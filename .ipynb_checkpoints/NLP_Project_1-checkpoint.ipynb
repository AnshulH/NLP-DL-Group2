{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AnshulH/NLP-DL-Group2/blob/fnn/NLP_Project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7U7h5_g6HQj",
    "outputId": "ad9f3202-90a3-4e78-b1ed-c1c39602fff3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: /content/NLP-DL-Group2/: No such file or directory\n",
      "zsh:cd:1: no such file or directory: /content\n"
     ]
    }
   ],
   "source": [
    "!rm -r /content/NLP-DL-Group2/\n",
    "!cd /content && git clone -b fnn https://github.com/AnshulH/NLP-DL-Group2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/derexwangmang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/derexwangmang/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/derexwangmang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words # added for detecting English words\n",
    "import string\n",
    "import nltk\n",
    "import calendar\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6AI8wvm-Y2J",
    "outputId": "f9a3169b-017f-4975-ffc4-a6ae7e5b7474",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def decode(vocab,corpus):\n",
    "    \n",
    "    text = ''\n",
    "    for i in range(len(corpus)):\n",
    "        wID = corpus[i]\n",
    "        text = text + vocab[wID] + ' '\n",
    "    return(text)\n",
    "\n",
    "def encode(words,text):\n",
    "    corpus = []\n",
    "    tokens = text.split(' ')\n",
    "    for t in tokens:\n",
    "        try:\n",
    "            wID = words[t][0]\n",
    "        except:\n",
    "            wID = words['<unk>'][0]\n",
    "        corpus.append(wID)\n",
    "    return(corpus)\n",
    "\n",
    "def read_encode(file_name,vocab,words,corpus,threshold):\n",
    "    \n",
    "    wID = len(vocab)\n",
    "    \n",
    "    if threshold > -1:\n",
    "        with open(file_name,'rt') as f:\n",
    "            for line in f:\n",
    "                line = line.replace('\\n','')\n",
    "                tokens = line.split(' ')\n",
    "                for t in tokens:\n",
    "                    try:\n",
    "                        elem = words[t]\n",
    "                    except:\n",
    "                        elem = [wID,0]\n",
    "                        vocab.append(t)\n",
    "                        wID = wID + 1\n",
    "                    elem[1] = elem[1] + 1\n",
    "                    words[t] = elem\n",
    "\n",
    "        temp = words\n",
    "        words = {}\n",
    "        vocab = []\n",
    "        wID = 0\n",
    "        words['<unk>'] = [wID,100]\n",
    "        vocab.append('<unk>')\n",
    "        for t in temp:\n",
    "            if temp[t][1] >= threshold:\n",
    "                vocab.append(t)\n",
    "                wID = wID + 1\n",
    "                words[t] = [wID, temp[t][1]]\n",
    "            \n",
    "                    \n",
    "    with open(file_name,'rt') as f:\n",
    "        for line in f:\n",
    "            line = line.replace('\\n','')\n",
    "            tokens = line.split(' ')\n",
    "            for t in tokens:\n",
    "                try:\n",
    "                    wID = words[t][0]\n",
    "                except:\n",
    "                    wID = words['<unk>'][0]\n",
    "                corpus.append(int(wID))\n",
    "                \n",
    "    return [vocab,words,corpus]\n",
    "\n",
    "def make_tensors(file_name,sequence_length):\n",
    "  '''\n",
    "  creates a list of tuples containing a list with the tokenized bio and a fake/real label\n",
    "\n",
    "  inputs: \n",
    "    file_name = a complete file path to a mixed dataset containing text and labels\n",
    "    sequence_length = the length of the desired input array\n",
    "  outputs:\n",
    "    x = a tensor containing wIDs of the desired sequence length for each bio\n",
    "    y = a tensor containing the corresponding label for each input\n",
    "  '''\n",
    "  \n",
    "  stop = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "\n",
    "  with open(file_name) as input:\n",
    "    all_text = input.readlines()\n",
    "\n",
    "# if anybody wants to do some work on the tokenization section, \n",
    "# you should work with the split_bios array, since it contains actual words\n",
    "\n",
    "# split_bios = [([bio1, tokens1],[REAL]),([bio2, tokens2],[FAKE])]\n",
    "  split_bios = []\n",
    "  line_text = []\n",
    "\n",
    "  empty = True\n",
    "\n",
    "  # Gets rid of first few empty lines of file\n",
    "  for i, line in enumerate(all_text):\n",
    "        line = line.strip()\n",
    "        if line != \"\":\n",
    "            empty = False\n",
    "            all_text = all_text[i:]\n",
    "            break\n",
    "\n",
    "  # seq_status = 0 marks the start of the bio\n",
    "  # seq_status = 1 is the main text of the bio\n",
    "  # seq_status = 2 is the end of the bio\n",
    "  for line in all_text:\n",
    "    line = line.strip()\n",
    "    if \"<start_bio>\" in line:\n",
    "      seq_status = 0\n",
    "      continue\n",
    "    if \"<end_bio>\" in line:\n",
    "      seq_status = 2\n",
    "      continue\n",
    "    if seq_status == 0:\n",
    "      bio_person = line.strip(\" = \").lower().split()\n",
    "      stop.update(bio_person) # could remove important words if name is meaningful\n",
    "      seq_status = 1\n",
    "    if seq_status == 1:\n",
    "      line = re.sub(\"[\\d-]\", '',line)\n",
    "      line = [token for token in word_tokenize(line.lower()) if token not in stop]\n",
    "      if len(line) > 0:\n",
    "        line_text.extend(line)\n",
    "    if seq_status == 2:\n",
    "      stop -= set(bio_person)\n",
    "      if line == '[FAKE]': #updated code so now fake / real is binary\n",
    "        line = 0\n",
    "      elif line == '[REAL]':\n",
    "        line = 1\n",
    "      else:\n",
    "        continue\n",
    "      split_bios.append((line_text,line))\n",
    "      line_text = []\n",
    "\n",
    "  encodings = read_encode(file_name,[],{},[],3)\n",
    "  word_dict = encodings[1]\n",
    "  vocab_length = max(encodings[2])\n",
    "\n",
    "  bio_array = []\n",
    "  seq_len = sequence_length\n",
    "  y = []\n",
    "\n",
    "  for bio in split_bios:\n",
    "    wid = []\n",
    "    for token in bio[0]:\n",
    "      word_info = word_dict.get(token)\n",
    "      if word_info is None:\n",
    "        wid.append(0)\n",
    "      else:\n",
    "        wid.append(word_info[0])\n",
    "    if len(wid) > seq_len:\n",
    "      wid = wid[:seq_len]\n",
    "    elif len(wid) < seq_len:\n",
    "      pad_size = seq_len - len(wid)\n",
    "      wid.extend(np.ones(pad_size,dtype=int))\n",
    "    bio_array.append(np.array(wid))\n",
    "    y.append(bio[1])\n",
    "\n",
    "  x = torch.tensor(bio_array)\n",
    "  y = torch.tensor(y)\n",
    "\n",
    "  return (x,y)\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, vocab, words, d_model, d_hidden, dropout):\n",
    "        super().__init__() \n",
    "    \n",
    "        self.vocab = vocab\n",
    "        self.words = words\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.d_model = d_model\n",
    "        self.d_hidden = d_hidden\n",
    "        self.dropout = dropout\n",
    "        self.embeds = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.linear1 = nn.Linear(50, 128)\n",
    "        self.nonLinear1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        self.nonLinear2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(256, 2)\n",
    "        self.logits = nn.Softmax()\n",
    "\n",
    "    def forward(self, src):\n",
    "#         embeds = self.dropout(self.embeds(src))\n",
    "        embeds = self.embeds(src)\n",
    "#         print(\"Shape starting out: \", embeds.shape)\n",
    "        output = self.linear1(embeds)\n",
    "        output = self.nonLinear2(output)\n",
    "        output = self.linear2(output)\n",
    "        output = self.nonLinear2(output)\n",
    "        output = self.linear3(output)\n",
    "#         print(\"Shape before logits: \", output.shape)\n",
    "        output = self.logits(output)\n",
    "#         output = self.logits(output)\n",
    "#         print(\"Shape before logits: \", output.shape)\n",
    "        return output\n",
    "\n",
    "#          {add code to implement the FFNN}\n",
    "        # return self.net(embeds)\n",
    "                \n",
    "    def init_weights(self):\n",
    "#          {perform initializations}\n",
    "        pass\n",
    "        \n",
    "        \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self,vocab,words,d_model,d_hidden,n_layers,dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.words = words\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.n_layers = n_layers\n",
    "        self.d_hidden = d_hidden\n",
    "        self.d_model = d_model\n",
    "        self.embeds = nn.Embedding(self.vocab_size, d_model)\n",
    "#          {perform other initializations needed for the LSTM}\n",
    "        \n",
    "    def forward(self,src,h):\n",
    "        embeds = self.dropout(self.embeds(src))\n",
    "#          {add code to implement the LSTM}     \n",
    "        pass\n",
    "        return [preds,h]\n",
    "    \n",
    "    def init_weights(self):\n",
    "#          {perform initializations}\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def detach_hidden(self, hidden):\n",
    "#          {needed for training...}\n",
    "        pass\n",
    "        return [hidden, cell]  \n",
    "    \n",
    "def main():\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-d_model', type=int, default=100)\n",
    "    parser.add_argument('-d_hidden', type=int, default=100)\n",
    "    parser.add_argument('-n_layers', type=int, default=2)\n",
    "    parser.add_argument('-batch_size', type=int, default=20)\n",
    "    parser.add_argument('-seq_len', type=int, default=30)\n",
    "    parser.add_argument('-printevery', type=int, default=5000)\n",
    "    parser.add_argument('-window', type=int, default=3)\n",
    "    parser.add_argument('-epochs', type=int, default=20)\n",
    "    parser.add_argument('-lr', type=float, default=0.0001)\n",
    "    parser.add_argument('-dropout', type=int, default=0.35)\n",
    "    parser.add_argument('-clip', type=int, default=2.0)\n",
    "    parser.add_argument('-model', type=str,default='LSTM')\n",
    "    parser.add_argument('-savename', type=str,default='lstm')\n",
    "    parser.add_argument('-loadname', type=str)\n",
    "    parser.add_argument('-trainname', type=str, default='/content/NLP-DL-Group2/hw#1/mix.train.txt')\n",
    "    parser.add_argument('-validname', type=str, default='/content/NLP-DL-Group2/hw#1/mix.valid.txt')\n",
    "    parser.add_argument('-testname', type=str, default='/content/NLP-DL-Group2/hw#1/mix.test.txt')\n",
    "    parser.add_argument(\"-f\", required=False)\n",
    "    # print(parser)\n",
    "    # params = vars(parser.parse_args())\n",
    "    params = {\n",
    "        'd_model': 100,\n",
    "        'd_hidden': 100,\n",
    "        'n_layers': 2,\n",
    "        'batch_size': 20,\n",
    "        'seq_len': 30,\n",
    "        'printevery': 5000,\n",
    "        'window': 3,\n",
    "        'epochs': 20,\n",
    "        'lr': 0.0001,\n",
    "        'dropout': 0.35,\n",
    "        'clip': 2.0,\n",
    "        'model': 'FFNN',\n",
    "        'savename': 'lstm',\n",
    "        'loadname': None,\n",
    "        'trainname': 'hw#1/mix.train.txt',\n",
    "        'validname': 'hw#1/mix.valid.txt',\n",
    "        'testname': 'hw#1/mix.test.txt'\n",
    "    }   \n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    [vocab,words,train] = read_encode(params['trainname'],[],{},[],3)\n",
    "    print('vocab: %d train: %d' % (len(vocab), len(train)))\n",
    "    [vocab,words,test] = read_encode(params['testname'], vocab,words,[],-1)\n",
    "    print('vocab: %d test: %d' % (len(vocab),len(test)))\n",
    "    params['vocab_size'] = len(vocab)\n",
    "    \n",
    "    epochs = 10\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if params['model'] == 'FFNN':\n",
    "        model = FFNN(vocab, words, 50, None, 0.1)\n",
    "        for i in range(epochs):\n",
    "            model.train()\n",
    "\n",
    "            output = model(torch.tensor(train, dtype=torch.long))\n",
    "            loss = criterion(output, torch.tensor([1] * 50))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "            optimizer.step()\n",
    "\n",
    "            print(\"Epoch {}/{} Step {}/{} : Loss {:.4f}\".format(1,1,i+1, 1,loss))\n",
    "\n",
    "\n",
    "#          {add code to instantiate the model, train for K epochs and save model to disk}\n",
    "        \n",
    "#     if params.model == 'LSTM':\n",
    "# #          {add code to instantiate the model, train for K epochs and save model to disk}\n",
    "\n",
    "#     if params.model == 'FFNN_CLASSIFY':\n",
    "# #          {add code to instantiate the model, recall model parameters and perform/learn classification}\n",
    "\n",
    "#     if params.model == 'LSTM_CLASSIFY':\n",
    "# #          {add code to instantiate the model, recall model parameters and perform/learn classification}\n",
    "        \n",
    "#     print(params)\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     main()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: 44223 train: 3366260\n",
      "vocab: 44223 test: 441210\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "        'd_model': 100,\n",
    "        'd_hidden': 100,\n",
    "        'n_layers': 2,\n",
    "        'batch_size': 20,\n",
    "        'seq_len': 30,\n",
    "        'printevery': 5000,\n",
    "        'window': 3,\n",
    "        'epochs': 20,\n",
    "        'lr': 0.0001,\n",
    "        'dropout': 0.35,\n",
    "        'clip': 2.0,\n",
    "        'model': 'FFNN',\n",
    "        'savename': 'lstm',\n",
    "        'loadname': None,\n",
    "        'trainname': 'hw#1/mix.train.txt',\n",
    "        'validname': 'hw#1/mix.valid.txt',\n",
    "        'testname': 'hw#1/mix.test.txt'\n",
    "    }\n",
    "\n",
    "torch.manual_seed(0)\n",
    "    \n",
    "[vocab,words,train] = read_encode(params['trainname'],[],{},[],3)\n",
    "print('vocab: %d train: %d' % (len(vocab), len(train)))\n",
    "[vocab,words,test] = read_encode(params['testname'], vocab,words,[],-1)\n",
    "print('vocab: %d test: %d' % (len(vocab),len(test)))\n",
    "params['vocab_size'] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pj/ljy6zhxs0k35pbb5gyhtmmb80000gn/T/ipykernel_22672/2564539889.py:155: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  x = torch.tensor(bio_array)\n"
     ]
    }
   ],
   "source": [
    "word_tensors = make_tensors(\"hw#1/mix.train.txt\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/51735001/how-to-include-batch-size-in-pytorch-basic-example\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "dataset = TensorDataset(word_tensors[0], word_tensors[1])\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkyuComqsH5i",
    "outputId": "fef12979-e103-4720-f839-62fb029f7c6d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pj/ljy6zhxs0k35pbb5gyhtmmb80000gn/T/ipykernel_22672/2564539889.py:188: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = self.logits(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100: Loss 0.6931\n",
      "Epoch 10/100: Loss 1.3133\n",
      "Epoch 20/100: Loss -0.0000\n",
      "Epoch 30/100: Loss -0.0000\n",
      "Epoch 40/100: Loss 0.3133\n",
      "Epoch 50/100: Loss 1.3133\n",
      "Epoch 60/100: Loss -0.0000\n",
      "Epoch 70/100: Loss 1.3133\n",
      "Epoch 80/100: Loss 0.6931\n",
      "Epoch 90/100: Loss 1.3133\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = FFNN(vocab, words, BATCH_SIZE, None, 0.1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(epochs):    \n",
    "    for id_batch, (x_batch, y_batch) in enumerate(dataloader):    \n",
    "        y_batch_predictions = model(x_batch)\n",
    "        y_batch_predictions_binary = []\n",
    "        # Replace with counting\n",
    "        \n",
    "        for _, row in enumerate(y_batch_predictions):\n",
    "            y_batch_predictions_temp = torch.argmax(row, 1)\n",
    "            y_batch_predictions_binary.append(torch.count_nonzero(y_batch_predictions_temp) >= 50)\n",
    "            \n",
    "        y_batch_predictions_binary = torch.Tensor(y_batch_predictions_binary).to(torch.float64)\n",
    "        \n",
    "        loss = criterion(y_batch_predictions_binary, y_batch.to(torch.float64))\n",
    "        loss.requires_grad = True\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    if i % 10 == 0:\n",
    "        print(\"Epoch {}/{}: Loss {:.4f}\".format(i, epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensors = make_tensors(\"hw#1/mix.test.txt\", 100)\n",
    "\n",
    "dataset = TensorDataset(test_tensors[0], test_tensors[1])\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pj/ljy6zhxs0k35pbb5gyhtmmb80000gn/T/ipykernel_22672/2564539889.py:188: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = self.logits(output)\n"
     ]
    }
   ],
   "source": [
    "y_predictions = []\n",
    "y_true = []\n",
    "    \n",
    "for id_batch, (x_batch, y_batch) in enumerate(dataloader):    \n",
    "    y_batch_predictions = model(x_batch)\n",
    "    y_batch_predictions_binary = []\n",
    "    # Replace with counting\n",
    "\n",
    "    for _, row in enumerate(y_batch_predictions):\n",
    "        y_batch_predictions_temp = torch.argmax(row, 1)\n",
    "        y_batch_predictions_binary.append(torch.count_nonzero(y_batch_predictions_temp) >= 50)\n",
    "\n",
    "    y_batch_predictions_binary = torch.Tensor(y_batch_predictions_binary).to(torch.float64)\n",
    "    y_predictions.extend(y_batch_predictions_binary.tolist())\n",
    "    y_true.extend(y_batch.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[366, 181],\n",
       "       [318, 187]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_true, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5256653992395437"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true, y_predictions)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyObKsdRWKVL6N2KJ+WBSuSJ",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
