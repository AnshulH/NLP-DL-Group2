{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AnshulH/NLP-DL-Group2/blob/lstm/NLP_Project_1_LSTM_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7U7h5_g6HQj",
    "outputId": "c7d6ce83-5032-4c05-e5c3-06e9774021ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/content/NLP-DL-Group2/': No such file or directory\n",
      "Cloning into 'NLP-DL-Group2'...\n",
      "remote: Enumerating objects: 57, done.\u001b[K\n",
      "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
      "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
      "remote: Total 57 (delta 2), reused 6 (delta 1), pack-reused 46\u001b[K\n",
      "Unpacking objects: 100% (57/57), 30.20 MiB | 2.69 MiB/s, done.\n",
      "Updating files: 100% (25/25), done.\n"
     ]
    }
   ],
   "source": [
    "!rm -r /content/NLP-DL-Group2/\n",
    "!cd /content && git clone https://github.com/AnshulH/NLP-DL-Group2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwOAM-Ht6WMu",
    "outputId": "4fdc04d3-b86a-45f9-e7b1-9c3eccd5b552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: 35151 train: 3012820\n",
      "vocab: 35151 test: 3012820\n",
      "[('<unk>', [0, 100]), ('<pad>', [1, 100]), ('<', [2, 15939]), ('start_bio', [3, 7966]), ('>', [4, 15944]), ('=', [5, 15861]), ('hildebrand', [6, 16]), ('bothe', [7, 11]), ('(', [8, 41269]), ('2', [9, 1725]), ('october', [10, 1605]), ('1781', [11, 67]), ('â€“', [12, 7704]), ('25', [13, 748]), ('september', [14, 1516])]\n",
      "{'d_model': 100, 'd_hidden': 100, 'n_layers': 2, 'batch_size': 20, 'seq_len': 30, 'printevery': 5000, 'window': 3, 'epochs': 20, 'lr': 0.0001, 'dropout': 0.35, 'clip': 2.0, 'model': 'LSTM', 'savename': 'lstm', 'loadname': None, 'trainname': '/content/NLP-DL-Group2/hw#1/mix.train.tok', 'validname': '/content/NLP-DL-Group2/hw#1/mix.train.tok', 'testname': '/content/NLP-DL-Group2/hw#1/mix.train.tok', 'vocab_size': 35151}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "def decode(vocab,corpus):\n",
    "    \n",
    "    text = ''\n",
    "    for i in range(len(corpus)):\n",
    "        wID = corpus[i]\n",
    "        text = text + vocab[wID] + ' '\n",
    "    return(text)\n",
    "\n",
    "def encode(words,text):\n",
    "    corpus = []\n",
    "    tokens = text.split(' ')\n",
    "    for t in tokens:\n",
    "        try:\n",
    "            wID = words[t][0]\n",
    "        except:\n",
    "            wID = words['<unk>'][0]\n",
    "        corpus.append(wID)\n",
    "    return(corpus)\n",
    "\n",
    "def read_encode(file_name,vocab,words,corpus,threshold):\n",
    "    \n",
    "    wID = len(vocab)\n",
    "    \n",
    "    if threshold > -1:\n",
    "        with open(file_name,'rt') as f:\n",
    "            for line in f:\n",
    "                line = line.replace('\\n','')\n",
    "                tokens = line.split(' ')\n",
    "                for t in tokens:\n",
    "                    try:\n",
    "                        elem = words[t]\n",
    "                    except:\n",
    "                        elem = [wID,0]\n",
    "                        vocab.append(t)\n",
    "                        wID = wID + 1\n",
    "                    elem[1] = elem[1] + 1\n",
    "                    words[t] = elem\n",
    "\n",
    "        temp = words\n",
    "        words = {}\n",
    "        vocab = []\n",
    "        wID = 0\n",
    "        words['<unk>'] = [wID,100]\n",
    "        wID = 1\n",
    "        words['<pad>'] = [wID,100]\n",
    "        vocab.append('<unk>')\n",
    "        vocab.append('<pad>')\n",
    "        for t in temp:\n",
    "            if temp[t][1] >= threshold:\n",
    "                vocab.append(t)\n",
    "                wID = wID + 1\n",
    "                words[t] = [wID,temp[t][1]]\n",
    "            \n",
    "                    \n",
    "    with open(file_name,'rt') as f:\n",
    "        for line in f:\n",
    "            line = line.replace('\\n','')\n",
    "            tokens = line.split(' ')\n",
    "            for t in tokens:\n",
    "                try:\n",
    "                    wID = words[t][0]\n",
    "                except:\n",
    "                    wID = words['<unk>'][0]\n",
    "                corpus.append(wID)\n",
    "                \n",
    "    return [vocab,words,corpus]\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, vocab, words,d_model, d_hidden, dropout):\n",
    "        super().__init__() \n",
    "    \n",
    "        self.vocab = vocab\n",
    "        self.words = words\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.d_model = d_model\n",
    "        self.d_hidden = d_hidden\n",
    "        self.dropout = dropout\n",
    "        self.embeds = nn.Embedding(self.vocab_size,d_model)\n",
    "#          {perform other initializations needed for the FFNN}\n",
    "\n",
    "    def forward(self, src):\n",
    "        embeds = self.dropout(self.embeds(src))\n",
    "#          {add code to implement the FFNN}\n",
    "        pass\n",
    "        # return x\n",
    "                \n",
    "    def init_weights(self):\n",
    "      pass\n",
    "#          {perform initializations}\n",
    "        \n",
    "        \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self,vocab,words,d_model,d_hidden,n_layers,dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.words = words\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.n_layers = n_layers\n",
    "        self.d_hidden = d_hidden\n",
    "        self.d_model = d_model\n",
    "        self.embeds = nn.Embedding(self.vocab_size,d_model)\n",
    "#          {perform other initializations needed for the LSTM}\n",
    "        self.lstm = nn.LSTM(d_model, d_hidden, n_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(d_hidden, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,src,h):\n",
    "        embeds = self.dropout(self.embeds(src))\n",
    "        batch_size = src.size(0)\n",
    "        src = src.long()\n",
    "        lstm_out, h = self.lstm(embeds, h)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.d_hidden)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, h\n",
    "    \n",
    "    def init_weights(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.d_hidden).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.d_hidden).zero_().to(device))\n",
    "        return hidden\n",
    "        \n",
    "    \n",
    "    def detach_hidden(self, hidden):\n",
    "      pass\n",
    "#          {needed for training...}\n",
    "        # return [hidden, cell]  \n",
    "    \n",
    "def main():\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-d_model', type=int, default=100)\n",
    "    parser.add_argument('-d_hidden', type=int, default=100)\n",
    "    parser.add_argument('-n_layers', type=int, default=2)\n",
    "    parser.add_argument('-batch_size', type=int, default=20)\n",
    "    parser.add_argument('-seq_len', type=int, default=30)\n",
    "    parser.add_argument('-printevery', type=int, default=5000)\n",
    "    parser.add_argument('-window', type=int, default=3)\n",
    "    parser.add_argument('-epochs', type=int, default=20)\n",
    "    parser.add_argument('-lr', type=float, default=0.0001)\n",
    "    parser.add_argument('-dropout', type=int, default=0.35)\n",
    "    parser.add_argument('-clip', type=int, default=2.0)\n",
    "    parser.add_argument('-model', type=str,default='LSTM')\n",
    "    parser.add_argument('-savename', type=str,default='lstm')\n",
    "    parser.add_argument('-loadname', type=str)\n",
    "    parser.add_argument('-trainname', type=str,default='wiki.train.txt')\n",
    "    parser.add_argument('-validname', type=str,default='wiki.valid.txt')\n",
    "    parser.add_argument('-testname', type=str,default='wiki.test.txt')\n",
    "\n",
    "    params = {\n",
    "        'd_model': 100,\n",
    "        'd_hidden': 100,\n",
    "        'n_layers': 2,\n",
    "        'batch_size': 20,\n",
    "        'seq_len': 30,\n",
    "        'printevery': 5000,\n",
    "        'window': 3,\n",
    "        'epochs': 20,\n",
    "        'lr': 0.0001,\n",
    "        'dropout': 0.35,\n",
    "        'clip': 2.0,\n",
    "        'model': 'LSTM',\n",
    "        'savename': 'lstm',\n",
    "        'loadname': None,\n",
    "        'trainname': '/content/NLP-DL-Group2/hw#1/mix.train.tok',\n",
    "        'validname': '/content/NLP-DL-Group2/hw#1/mix.train.tok',\n",
    "        'testname': '/content/NLP-DL-Group2/hw#1/mix.train.tok'\n",
    "    }\n",
    "    parser.add_argument(\"-f\", required=False)\n",
    "    \n",
    "    # params = parser.parse_args()    \n",
    "    # torch.manual_seed(0)\n",
    "    \n",
    "    [vocab,words,train] = read_encode(params['trainname'],[],{},[],3)\n",
    "    print('vocab: %d train: %d' % (len(vocab),len(train)))\n",
    "    [vocab,words,test] = read_encode(params['testname'],vocab,words,[],-1)\n",
    "    print('vocab: %d test: %d' % (len(vocab),len(test)))\n",
    "    params['vocab_size'] = len(vocab)\n",
    "\n",
    "    train_loader = read_encode(params['trainname'],[],{},[],3)\n",
    "    \n",
    "    if params['model'] == 'FFNN':\n",
    "      pass\n",
    "#          {add code to instantiate the model, train for K epochs and save model to disk}\n",
    "        \n",
    "    if params['model'] == 'LSTM':\n",
    "      model = LSTM(vocab,words,params['d_model'],params['d_hidden'],params['n_layers'],params['dropout'])\n",
    "      model.to(device)\n",
    "\n",
    "      lr=0.005\n",
    "      criterion = nn.BCELoss()\n",
    "      optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "      epochs = 2\n",
    "      counter = 0\n",
    "      print_every = 1000\n",
    "      clip = 5\n",
    "      valid_loss_min = np.Inf\n",
    "\n",
    "      model.train()\n",
    "      print(list(train_loader[1].items())[0:15])\n",
    "\n",
    "      # for i in range(epochs):\n",
    "      #   h = model.init_weights(params['batch_size'])\n",
    "        \n",
    "      #   for vocab, labels in oops:\n",
    "      #       counter += 1\n",
    "      #       h = tuple([e.data for e in h])\n",
    "      #       vocab, labels = vocab.to(device), labels.to(device)\n",
    "      #       model.zero_grad()\n",
    "      #       output, h = model(vocab, h)\n",
    "      #       loss = criterion(output.squeeze(), labels.float())\n",
    "      #       loss.backward()\n",
    "      #       nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "      #       optimizer.step()\n",
    "            \n",
    "      #       if counter%print_every == 0:\n",
    "      #           val_h = model.init_hidden(read_encode(params['validname'],[],{},[],3))\n",
    "      #           val_losses = []\n",
    "      #           model.eval()\n",
    "      #           for inp, lab in params['validname']:\n",
    "      #               val_h = tuple([each.data for each in val_h])\n",
    "      #               inp, lab = inp.to(device), lab.to(device)\n",
    "      #               out, val_h = model(inp, val_h)\n",
    "      #               val_loss = criterion(out.squeeze(), lab.float())\n",
    "      #               val_losses.append(val_loss.item())\n",
    "                    \n",
    "      #           model.train()\n",
    "      #           print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "      #                 \"Step: {}...\".format(counter),\n",
    "      #                 \"Loss: {:.6f}...\".format(loss.item()),\n",
    "      #                 \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "      #           if np.mean(val_losses) <= valid_loss_min:\n",
    "      #               torch.save(model.state_dict(), './state_dict.pt')\n",
    "      #               print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "      #               valid_loss_min = np.mean(val_losses)\n",
    "\n",
    "    if params['model'] == 'FFNN_CLASSIFY':\n",
    "      pass\n",
    "#          {add code to instantiate the model, recall model parameters and perform/learn classification}\n",
    "\n",
    "    if params['model'] == 'LSTM_CLASSIFY':\n",
    "      pass\n",
    "#          {add code to instantiate the model, recall model parameters and perform/learn classification}\n",
    "        \n",
    "    print(params)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3BjxK9QyXLA",
    "outputId": "0f9ed66c-41cf-435e-d383-e84a08a82828"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words # added for detecting English words\n",
    "import string\n",
    "import nltk\n",
    "import calendar\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "LYnopIlVDnnc"
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english') + list(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RClc0OzADqLo"
   },
   "outputs": [],
   "source": [
    "original_file = '/content/NLP-DL-Group2/hw#1/mix.train.txt'\n",
    "tokenized_file = '/content/NLP-DL-Group2/hw#1/mix_tokenized.train.txt'\n",
    "\n",
    "with open(original_file) as input:\n",
    "  with open(tokenized_file, \"w\") as output:\n",
    "    curr_bio = []\n",
    "    for line in input:\n",
    "      line_stripped = line.strip()\n",
    "      if line_stripped != '':\n",
    "        if line_stripped == \"\":\n",
    "          bio_person = [token for token in word_tokenize(next(input).lower()) if token != \"=\"]\n",
    "          \n",
    "          # Adding name of person as stop word\n",
    "          stop.update(bio_person)\n",
    "        elif line_stripped == \"\":\n",
    "          updated = ' '.join([token for token in curr_bio])\n",
    "          output.write(updated)\n",
    "          print(updated)\n",
    "          curr_bio = []\n",
    "\n",
    "          # Removing name of person as stop word\n",
    "          stop -= set(bio_person)\n",
    "          break\n",
    "        else:\n",
    "          curr_bio.extend([token for token in word_tokenize(line_stripped.lower()) if token not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "_oZAoBCxE8Ht"
   },
   "outputs": [],
   "source": [
    "# create a list of tuples containing a list with the tokenized bio and a fake/real label\n",
    "# split_bios = [([bio1, tokens1],[REAL]),([bio2, tokens2],[FAKE])]\n",
    "\n",
    "original_file = '/content/NLP-DL-Group2/hw#1/mix.train.txt'\n",
    "\n",
    "with open(original_file) as input:\n",
    "  all_text = input.readlines()\n",
    "\n",
    "split_bios = []\n",
    "line_text = []\n",
    "\n",
    "# seq_status = 0 marks the start of the bio\n",
    "# seq_status = 1 is the main text of the bio\n",
    "# seq_status = 2 is the end of the bio\n",
    "for line in all_text:\n",
    "  line = line.strip()\n",
    "  if \"<start_bio>\" in line:\n",
    "    seq_status = 0\n",
    "    continue\n",
    "  if \"<end_bio>\" in line:\n",
    "    seq_status = 2\n",
    "    continue\n",
    "  if seq_status == 0:\n",
    "    bio_person = line.strip(\" = \").lower().split()\n",
    "    stop.update(bio_person) # could remove important words if name is meaningful\n",
    "    seq_status = 1\n",
    "    continue\n",
    "  if seq_status == 1:\n",
    "    line = re.sub(\"[\\d-]\", '',line)\n",
    "    line = [token for token in word_tokenize(line.lower()) if token not in stop]\n",
    "    if len(line) > 0:\n",
    "      line_text.extend(line)\n",
    "  if seq_status == 2 and '[' in line:\n",
    "    stop -= set(bio_person)\n",
    "    split_bios.append((line_text,line))\n",
    "    line_text = []\n",
    "\n",
    "# print(split_bios[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3apfnestaJt9",
    "outputId": "24a6bbff-0ff8-4c18-b36a-3c9504837e03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  8,  10,  12,  14,  16,  17,  18,  19,  20,  17,  22,  23,  24,\n",
      "        25,  26,  27,  23,  28,  25,  29,  31,  18,  32,  33,  34,  35,\n",
      "        36,  37,   0,  36,  38,  23,  17,  40,  41,  37,  42,  43,  44,\n",
      "        37,  46,  42,  43,  47,  49,  51,  18,  52,  53,  25,  54,  55,\n",
      "        56,  18,  57,  36,  37,  58,  59,  37,  58,  60,  24,  23,  62,\n",
      "        18,  63,  64,  33,  37,  65,  66,  25,  67,  33,  37,  68,   0,\n",
      "        23,  70,  72,  55,  73,  18,  74,  75,  34,  77,  81,  83,  84,\n",
      "        85,  33,  86,  87,  52,  88,  89,   0,   8,   0,   0,  16,  90,\n",
      "         0,   8,   0,  91,  16,  25,   0,  92,   8,   0,   0,  16,  93,\n",
      "        23,  24,  33,  14,  95,  96,  95,   0,  25,  37,   0,  97,  37,\n",
      "        19,  98,  99,   8,   0,  16,  18,  19, 100, 101, 102, 104, 105,\n",
      "       106, 100,   0,   0,   0,   0,  25,   0,   8,  24,  16,  37,   0,\n",
      "        66,   8, 108, 109,   8,  38,  12,  16,  70,  46,   0, 111, 109,\n",
      "        12,  16, 100,   0,   8,  38,  12,  16, 100, 114, 115,   8,  46,\n",
      "        16,   0,  23, 117,   0, 118,   8,  28,  16,  97,  19, 100,   8,\n",
      "       120,  38,   0, 111,  12,  16,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1]), array([  8,  14,  12, 129,  16,  17,  18, 131, 132, 133, 134,  36,  37,\n",
      "         0, 136,   8,  36,  37, 137, 138,  16,  17,  18, 139, 140, 141,\n",
      "       134,  33,  37,   0, 142,  25, 143, 142, 144, 146, 147, 148, 149,\n",
      "       150, 151,  17,  33,   0, 153,  23,  37,   0,  25,  87,   0,  17,\n",
      "        18, 154,  37, 131, 155, 156,   0, 136,  36, 138,   8, 157, 158,\n",
      "        70, 160,  37, 137, 138,  16,  17, 161,  23, 149, 162,  56,  17,\n",
      "        86, 164,  95, 165,  95, 166, 167, 168,   0, 169, 170, 171, 172,\n",
      "       173, 174,   0, 175, 168,   0, 176, 172, 177,   0, 178, 179,   0,\n",
      "       180, 181, 177,   0, 184, 185,   0,  23, 186,   0, 174,   0, 187,\n",
      "       179, 188,   8,  16, 425, 172, 177,   0,  23, 177, 192, 193, 194,\n",
      "       195, 196, 187, 179, 188,   8,  16, 425, 172, 177, 197,   0,  23,\n",
      "       177, 192, 193, 194, 195, 196, 187, 179, 188,   8,  16, 425, 172,\n",
      "       177, 198,   0, 199, 185,   0, 187, 179, 188,   8,  16, 425, 177,\n",
      "         0, 174,   0, 200, 201, 202,   0, 187, 179, 188,   8,  16, 425,\n",
      "       172, 177, 204,   0, 174,   0,   0, 205, 206, 180, 181,  12, 172,\n",
      "       177, 166, 167,   8,  23, 131,  16, 171, 172, 173, 174,   0,  23,\n",
      "       177, 192, 193, 194, 195, 196, 187, 179, 188,   8,  16, 425, 172,\n",
      "       177, 185,   0, 209, 211, 212, 213, 214, 188,   8,  31, 216,   0,\n",
      "        16, 217, 218, 219, 220, 221, 219, 188, 179,   0, 187, 222,  41,\n",
      "       223,   0,  37, 224, 225, 226, 227,   8, 228,  16, 229,  37, 230,\n",
      "       149, 231, 232,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1]), array([    8,   236,    12,   238,    16,    17,   205,   240,   241,\n",
      "         242,   243,   149,    55,    33,   244,   245,   246,   149,\n",
      "          55,    33,   247,    17,   248,    37,   249,    96,   250,\n",
      "         247,    23,    37, 19022,   252,    95,   253,    95,    17,\n",
      "          22,    23,     0,   254,   255,     0, 15390,   257,   259,\n",
      "          23,   260,   261,   149,   262,    23,   265,     0,    51,\n",
      "          18,   266,    36,    37,   267,   268,   136,    23,   257,\n",
      "          23,    25,   271,   134,    36,    37,   272,   254,     0,\n",
      "         273,   274,   275,   276,    23,   278,    37,   279,   280,\n",
      "          23,   141,   243,   149,    55,    33,   247,   273,    37,\n",
      "           0,   282,    55,    36,    37,   267,   268,   136,   283,\n",
      "         284,     0,    25,    17,   288,    37,   136,   289,    25,\n",
      "         290,    37,   272,   254,     0,    23,    17,   292,    56,\n",
      "          18,   293,    37,   294,    47,    46,    25,    23,    51,\n",
      "          18,   293,    37,   294,    47,   296,    23,    51,   205,\n",
      "         298,    37,   267,   151,   299,   296,    23,    51,    37,\n",
      "         301,   240,   154,    37,   302,   299,   243,    37,   303,\n",
      "         304,   305,     8,     0,    16,    23,    51,    37,   301,\n",
      "         307,   308,   309,    56,   310,    23,    37,   311,   312,\n",
      "         313,    37,   299,   243,    37,   314,   315,   316,    56,\n",
      "          18,   154,    37,   314,   315,   317,   318,    33,   305,\n",
      "         319,    17,    37,   301,   322,   323,    25,   324,   244,\n",
      "         325,    23,   250,   247,   326,    37,   327,     0,    23,\n",
      "          37,   328,   329,    25,   330,   331,    56,    18,   332,\n",
      "           0,   273,    37,   333,   335,    37,   336,   337,    25,\n",
      "         338,    55,    31,    37,   339,    25,   340,   341,    37,\n",
      "         342,    25,   343,   344,    37,     0,   132,   345,   346,\n",
      "         347,   149,   348,    33,   250,   247,    23,   149,   350,\n",
      "           0,    51,    18,   154,    37,   240,   155,   352,    25,\n",
      "         316,    56,   310,   354,   355,   356,    37,   357,    47,\n",
      "          46,   359,   243,   149,     0,   151,    23,   247,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1]), array([    0,     8,    10,    12,   238,    16,    17,   205,   366,\n",
      "         367,   367,    37,   368,   134,    56,    18,   370,   371,\n",
      "       28579,    37,   373,     0,   373,   374,    25,    37,   375,\n",
      "         376,   377,    96,   378,    37,   379,   380,   381,   243,\n",
      "         382,   383,   385,    37,   386,   134,    56,    18,   370,\n",
      "         371,    25,    17,   387,   388,   389,   390,     0,    17,\n",
      "          22,    23,   391,   379,   393,   394,   395,   396,    17,\n",
      "         205,   371,   133,   397,    36,    37,   137,   379,   398,\n",
      "          23,    25,    51,    37,   137,   379,   396,   401,   402,\n",
      "         137,   403,    37,   158,   370,    36,   404,    23,   406,\n",
      "          23,   408,    18,   409,    23,   370,   410,     0,   278,\n",
      "         411,   370,   412,   402,   137,    23,    95,    96,    95,\n",
      "          23,     0,    85,     0,     0,   415,    23,    18,   416,\n",
      "         417,   418,   386,    31,   419,    41,   420,     0,    17,\n",
      "          37,   375,   421,   401,    33,   422,   423,   424,   421,\n",
      "         425,   427,   428,    37,     0,   276,   430,   427,   432,\n",
      "         373,   433,   435,    18,   436,   424,   243,    18,   437,\n",
      "         393,   438,    25,   439,     0,   440,   276,     0,   276,\n",
      "         441,   243,   442,   331,   443,   444,   373,   445,   331,\n",
      "         446,   427,   276,   447,   448,   427,   447,   242,   373,\n",
      "         449,   427,    18,   451,    25,   452,   453,    37,   424,\n",
      "         454,    25,   455,    25,     0,    25,   456,    25,   457,\n",
      "         373,   458,   248,   446,   459,   460,   461,   243,   448,\n",
      "          37,   462,   464,   373,   465,   427,   466,   467,     0,\n",
      "          25,   466,   471,   473,    23,   474,     0,     0,   476,\n",
      "         243,   386,    25,   477,    37,    55,    17,   478,    33,\n",
      "          18,   479,     0,    41,   449,     0,     0,    17,   354,\n",
      "         205,   481,   482,   370,   412,    41,    37,   137,   483,\n",
      "         484,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "           1,     1,     1,     1,     1,     1,     1,     1]), array([ 485,    0,    8,  489,   23,    0,   12,  236,   23,  493,   16,\n",
      "         17,   37,  301,    0,   17,   37,  494,  495,  496,    0,  133,\n",
      "        498,    0,    0,  493,    0,    0,   25,    0,   25,  149,  500,\n",
      "        501,    0,   95,  502,  503,   95,  504,  149,  505,  506,   25,\n",
      "        507,  316,   23,   37,  508,  446,  509,   23,  511,  149,  512,\n",
      "         31,  513,    0,  485,   17,   84,  514,   23,   37,  515,  383,\n",
      "        275,  517,  518,  149,  519,  521,  485,  443,  522,  242,  243,\n",
      "        149,  523,  493,  524,   18,  525,  526,   37,  105,  526,   17,\n",
      "        527,   41,   37,  528,  529,  530,  531,  331,   17,  532,   23,\n",
      "         37,  493,  160,   37,  533,  534,  485,  278,   37,  533,   17,\n",
      "        535,  149,  393,  446,  536,   95,  512,   25,  537,   95,   23,\n",
      "        485,  387,  513,    0,   18,  539,  540,  394,   37,  541,  485,\n",
      "         25,  513,   37,  311,  542,  425,  543,    8,   16,  501,  544,\n",
      "          8,   14,   12,   14,   16,  387,  506,  548,    0,    8,  549,\n",
      "         16,   37,    0,    0,  551,   25,    0,   17,   37,  494,  552,\n",
      "          0,    8,  549,   16,   25,  554,    0,  551,    8, 2477,   16,\n",
      "        552,  556,    8,   14,   12,  559,   16,    0,    8,  559,   12,\n",
      "        236,   16,  387,  485,  564,    0,  513,    8,  565,   12,  566,\n",
      "         16,  387,  568,  276,    0,    8,  489,   23,  570,   12,  565,\n",
      "         23,  570,   16,   17,   37,  494,  568,  573,    0,    8,   16,\n",
      "         25,  149,  574,  500,  575,    0,    8,   16,    0,    8,  577,\n",
      "         12,  580,   16,  582,    8,  566,   12,  565,   16,  387,  586,\n",
      "        496,    0,    8, 2477,   16,  587,  588,    8,  489,   12,  129,\n",
      "         16,  387,  592,  593,  276,    0,    8,  594,   23,    0,   12,\n",
      "        238,   23,  596,   16,   17,   37,  494,  506,  597,    0,   25,\n",
      "          0,  598,    0,    8,  559,   23,    0,   12,  565,   23,    0,\n",
      "         16,  495,  496,    8,  594,   12,   14,   16,  543,  603,    8,\n",
      "        559,   12,  238,   16,  607,  608,    8,  238,   12,   10,   16,\n",
      "        387,  554,  612,  613,    0,   37,  451,  614,  276,  612,    0,\n",
      "         25,  575,    0,  506,  548,    8,  580,   12,  129,   16,  387,\n",
      "        501,  587,    0,    8,  129,   23,  618,   12,  238,   23,    0,\n",
      "         16,   17,   18,  451,  620,    0,    8,  489,   23,  618,   12,\n",
      "        129,   23,  618,   16,   25,  501,  622,    0,    8,   10,   23,\n",
      "          0,   12,  129,   23,    0,   16,  501,  622,   17,   37,  624,\n",
      "        451,  507,    0,  507,    8,  565,   12,   10,   16,  485,  548,\n",
      "          8,  580,   12,  129,   16,  575,    0,    8,   14,   12,   14,\n",
      "         16,  387,  506,  568,    0,    8,  489,   23,    0,   12,  236,\n",
      "         16,  133,   17,    0,  149,  536,   17,   18,  494,  552,  556,\n",
      "        276,    0,    8,   16,   25,  575,  634,    0,    8,   16,   95,\n",
      "        635,   95,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1])]\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "encodings = read_encode('/content/NLP-DL-Group2/hw#1/mix.train.tok',[],{},[],3)\n",
    "word_dict = encodings[1]\n",
    "vocab_length = max(encodings[2])\n",
    "\n",
    "bio_array = []\n",
    "seq_len = 512\n",
    "\n",
    "for bio in split_bios:\n",
    "  wid = []\n",
    "  for token in bio[0]:\n",
    "    word_info = word_dict.get(token)\n",
    "    if word_info is None:\n",
    "      wid.append(0)\n",
    "    else:\n",
    "      wid.append(word_info[0])\n",
    "  if len(wid) > seq_len:\n",
    "    wid = wid[:seq_len]\n",
    "  elif len(wid) < seq_len:\n",
    "    pad_size = seq_len - len(wid)\n",
    "    wid.extend(np.ones(pad_size,dtype=int))\n",
    "  bio_array.append(np.array(wid))\n",
    "\n",
    "list_len = [len(i) for i in bio_array]\n",
    "print(bio_array[:5])\n",
    "print(max(list_len))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FiAJRgfUl50Q",
    "outputId": "f8484f48-0d65-48f8-cf46-bec89229f8a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(4,dtype=int)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
