{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObKsdRWKVL6N2KJ+WBSuSJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnshulH/NLP-DL-Group2/blob/fnn/NLP_Project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7U7h5_g6HQj",
        "outputId": "ad9f3202-90a3-4e78-b1ed-c1c39602fff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP-DL-Group2'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 54 (delta 1), reused 6 (delta 1), pack-reused 46\u001b[K\n",
            "Unpacking objects: 100% (54/54), 30.20 MiB | 3.62 MiB/s, done.\n",
            "Updating files: 100% (14/14), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -r /content/NLP-DL-Group2/\n",
        "!cd /content && git clone -b fnn https://github.com/AnshulH/NLP-DL-Group2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import sys\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "def decode(vocab,corpus):\n",
        "    \n",
        "    text = ''\n",
        "    for i in range(len(corpus)):\n",
        "        wID = corpus[i]\n",
        "        text = text + vocab[wID] + ' '\n",
        "    return(text)\n",
        "\n",
        "def encode(words,text):\n",
        "    corpus = []\n",
        "    tokens = text.split(' ')\n",
        "    for t in tokens:\n",
        "        try:\n",
        "            wID = words[t][0]\n",
        "        except:\n",
        "            wID = words['<unk>'][0]\n",
        "        corpus.append(wID)\n",
        "    return(corpus)\n",
        "\n",
        "def read_encode(file_name,vocab,words,corpus,threshold):\n",
        "    \n",
        "    wID = len(vocab)\n",
        "    \n",
        "    if threshold > -1:\n",
        "        with open(file_name,'rt') as f:\n",
        "            for line in f:\n",
        "                line = line.replace('\\n','')\n",
        "                tokens = line.split(' ')\n",
        "                for t in tokens:\n",
        "                    try:\n",
        "                        elem = words[t]\n",
        "                    except:\n",
        "                        elem = [wID,0]\n",
        "                        vocab.append(t)\n",
        "                        wID = wID + 1\n",
        "                    elem[1] = elem[1] + 1\n",
        "                    words[t] = elem\n",
        "\n",
        "        temp = words\n",
        "        words = {}\n",
        "        vocab = []\n",
        "        wID = 0\n",
        "        words['<unk>'] = [wID,100]\n",
        "        vocab.append('<unk>')\n",
        "        for t in temp:\n",
        "            if temp[t][1] >= threshold:\n",
        "                vocab.append(t)\n",
        "                wID = wID + 1\n",
        "                words[t] = [wID, temp[t][1]]\n",
        "            \n",
        "                    \n",
        "    with open(file_name,'rt') as f:\n",
        "        for line in f:\n",
        "            line = line.replace('\\n','')\n",
        "            tokens = line.split(' ')\n",
        "            for t in tokens:\n",
        "                try:\n",
        "                    wID = words[t][0]\n",
        "                except:\n",
        "                    wID = words['<unk>'][0]\n",
        "                corpus.append(int(wID))\n",
        "                \n",
        "    return [vocab,words,corpus]\n",
        "\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, vocab, words, d_model, d_hidden, dropout):\n",
        "        super().__init__() \n",
        "    \n",
        "        self.vocab = vocab\n",
        "        self.words = words\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.d_model = d_model\n",
        "        self.d_hidden = d_hidden\n",
        "        self.dropout = dropout\n",
        "        self.embeds = nn.Embedding(self.vocab_size, d_model)\n",
        "        self.linear1 = nn.Linear(50, 128)\n",
        "        self.nonLinear1 = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(128, 256)\n",
        "        self.nonLinear2 = nn.ReLU()\n",
        "        self.linear3 = nn.Linear(256, 2)\n",
        "        self.logits = nn.Softmax()\n",
        "\n",
        "    def forward(self, src):\n",
        "        # print(src)\n",
        "        embeds = self.embeds(src[0:50])\n",
        "        # print(embeds)\n",
        "        # src = src.type(torch.LongTensor)\n",
        "        # print(type(src[0]))\n",
        "        output = self.linear1(embeds)\n",
        "        output = self.nonLinear2(output)\n",
        "        output = self.linear2(output)\n",
        "        output = self.nonLinear2(output)\n",
        "        output = self.linear3(output)\n",
        "        output = self.logits(output)\n",
        "        return output\n",
        "\n",
        "#          {add code to implement the FFNN}\n",
        "        # return self.net(embeds)\n",
        "                \n",
        "    def init_weights(self):\n",
        "#          {perform initializations}\n",
        "        pass\n",
        "        \n",
        "        \n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self,vocab,words,d_model,d_hidden,n_layers,dropout_rate):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.vocab = vocab\n",
        "        self.words = words\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.n_layers = n_layers\n",
        "        self.d_hidden = d_hidden\n",
        "        self.d_model = d_model\n",
        "        self.embeds = nn.Embedding(self.vocab_size, d_model)\n",
        "#          {perform other initializations needed for the LSTM}\n",
        "        \n",
        "    def forward(self,src,h):\n",
        "        embeds = self.dropout(self.embeds(src))\n",
        "#          {add code to implement the LSTM}     \n",
        "        pass\n",
        "        return [preds,h]\n",
        "    \n",
        "    def init_weights(self):\n",
        "#          {perform initializations}\n",
        "        pass\n",
        "        \n",
        "    \n",
        "    def detach_hidden(self, hidden):\n",
        "#          {needed for training...}\n",
        "        pass\n",
        "        return [hidden, cell]  \n",
        "    \n",
        "def main():\n",
        "    \n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-d_model', type=int, default=100)\n",
        "    parser.add_argument('-d_hidden', type=int, default=100)\n",
        "    parser.add_argument('-n_layers', type=int, default=2)\n",
        "    parser.add_argument('-batch_size', type=int, default=20)\n",
        "    parser.add_argument('-seq_len', type=int, default=30)\n",
        "    parser.add_argument('-printevery', type=int, default=5000)\n",
        "    parser.add_argument('-window', type=int, default=3)\n",
        "    parser.add_argument('-epochs', type=int, default=20)\n",
        "    parser.add_argument('-lr', type=float, default=0.0001)\n",
        "    parser.add_argument('-dropout', type=int, default=0.35)\n",
        "    parser.add_argument('-clip', type=int, default=2.0)\n",
        "    parser.add_argument('-model', type=str,default='LSTM')\n",
        "    parser.add_argument('-savename', type=str,default='lstm')\n",
        "    parser.add_argument('-loadname', type=str)\n",
        "    parser.add_argument('-trainname', type=str, default='/content/NLP-DL-Group2/hw#1/mix.train.txt')\n",
        "    parser.add_argument('-validname', type=str, default='/content/NLP-DL-Group2/hw#1/mix.valid.txt')\n",
        "    parser.add_argument('-testname', type=str, default='/content/NLP-DL-Group2/hw#1/mix.test.txt')\n",
        "    parser.add_argument(\"-f\", required=False)\n",
        "    # print(parser)\n",
        "    # params = vars(parser.parse_args())\n",
        "    params = {\n",
        "        'd_model': 100,\n",
        "        'd_hidden': 100,\n",
        "        'n_layers': 2,\n",
        "        'batch_size': 20,\n",
        "        'seq_len': 30,\n",
        "        'printevery': 5000,\n",
        "        'window': 3,\n",
        "        'epochs': 20,\n",
        "        'lr': 0.0001,\n",
        "        'dropout': 0.35,\n",
        "        'clip': 2.0,\n",
        "        'model': 'FFNN',\n",
        "        'savename': 'lstm',\n",
        "        'loadname': None,\n",
        "        'trainname': '/content/NLP-DL-Group2/hw#1/mix.train.txt',\n",
        "        'validname': '/content/NLP-DL-Group2/hw#1/mix.valid.txt',\n",
        "        'testname': '/content/NLP-DL-Group2/hw#1/mix.test.txt'\n",
        "    }   \n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    \n",
        "    [vocab,words,train] = read_encode(params['trainname'],[],{},[],3)\n",
        "    print('vocab: %d train: %d' % (len(vocab), len(train)))\n",
        "    [vocab,words,test] = read_encode(params['testname'], vocab,words,[],-1)\n",
        "    print('vocab: %d test: %d' % (len(vocab),len(test)))\n",
        "    params['vocab_size'] = len(vocab)\n",
        "    \n",
        "    epochs = 10\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if params['model'] == 'FFNN':\n",
        "        model = FFNN(vocab, words, 50, None, 0.1)\n",
        "        for i in range(epochs):\n",
        "            model.train()\n",
        "\n",
        "            output = model(torch.tensor(train, dtype=torch.long))\n",
        "            loss = criterion(output, torch.tensor([1] * 50))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "            optimizer.step()\n",
        "\n",
        "            print(\"Epoch {}/{} Step {}/{} : Loss {:.4f}\".format(1,1,i+1, 1,loss))\n",
        "\n",
        "\n",
        "#          {add code to instantiate the model, train for K epochs and save model to disk}\n",
        "        \n",
        "#     if params.model == 'LSTM':\n",
        "# #          {add code to instantiate the model, train for K epochs and save model to disk}\n",
        "\n",
        "#     if params.model == 'FFNN_CLASSIFY':\n",
        "# #          {add code to instantiate the model, recall model parameters and perform/learn classification}\n",
        "\n",
        "#     if params.model == 'LSTM_CLASSIFY':\n",
        "# #          {add code to instantiate the model, recall model parameters and perform/learn classification}\n",
        "        \n",
        "#     print(params)\n",
        "\n",
        "    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "V6AI8wvm-Y2J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a3169b-017f-4975-ffc4-a6ae7e5b7474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab: 44223 train: 3366260\n",
            "vocab: 44223 test: 441210\n",
            "Epoch 1/1 Step 1/1 : Loss 0.6869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-34fb0d826e53>:105: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.logits(output)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1 Step 2/1 : Loss 0.3470\n",
            "Epoch 1/1 Step 3/1 : Loss 0.3136\n",
            "Epoch 1/1 Step 4/1 : Loss 0.3133\n",
            "Epoch 1/1 Step 5/1 : Loss 0.3133\n",
            "Epoch 1/1 Step 6/1 : Loss 0.3133\n",
            "Epoch 1/1 Step 7/1 : Loss 0.3133\n",
            "Epoch 1/1 Step 8/1 : Loss 0.3133\n",
            "Epoch 1/1 Step 9/1 : Loss 0.3133\n",
            "Epoch 1/1 Step 10/1 : Loss 0.3133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "    \n",
        "[vocab,words,train] = read_encode(params['trainname'],[],{},[],3)\n",
        "print('vocab: %d train: %d' % (len(vocab), len(train)))\n",
        "[vocab,words,test] = read_encode(params['testname'], vocab,words,[],-1)\n",
        "print('vocab: %d test: %d' % (len(vocab),len(test)))\n",
        "params['vocab_size'] = len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3_JhTkyr_ub",
        "outputId": "46d23572-bf95-4030-a2b5-93e65a916ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab: 44223 train: 3366260\n",
            "vocab: 44223 test: 441210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "if params['model'] == 'FFNN':\n",
        "    model = FFNN(vocab, words, 50, None, 0.1)\n",
        "    for i in range(epochs):\n",
        "        model.train()\n",
        "\n",
        "        output = model(torch.tensor(train, dtype=torch.long))\n",
        "        loss = criterion(output, torch.tensor([1] * 50))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "        optimizer.step()\n",
        "\n",
        "        print(\"Epoch {}/{} Step {}/{} : Loss {:.4f}\".format(1,1,i+1, 1,loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkyuComqsH5i",
        "outputId": "fef12979-e103-4720-f839-62fb029f7c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-34fb0d826e53>:105: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  output = self.logits(output)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1 Step 1/1 : Loss 0.6760\n",
            "Epoch 1/1 Step 2/1 : Loss 0.3474\n",
            "Epoch 1/1 Step 3/1 : Loss 0.3135\n",
            "Epoch 1/1 Step 4/1 : Loss 0.3133\n",
            "Epoch 1/1 Step 5/1 : Loss 0.3133\n",
            "Epoch 1/1 Step 6/1 : Loss 0.3133\n",
            "Epoch 1/1 Step 7/1 : Loss 0.3133\n",
            "Epoch 1/1 Step 8/1 : Loss 0.3133\n",
            "Epoch 1/1 Step 9/1 : Loss 0.3133\n",
            "Epoch 1/1 Step 10/1 : Loss 0.3133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "jZm_sZpEsBuv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6631a302-dbaf-4dbc-92ba-02fa310ff763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop = set(stopwords.words('english') + list(string.punctuation))"
      ],
      "metadata": {
        "id": "YE8ofGcuC2zr"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_file = '/content/NLP-DL-Group2/hw#1/mix.train.txt'\n",
        "tokenized_file = '/content/NLP-DL-Group2/hw#1/mix_tokenized.train.txt'\n",
        "\n",
        "with open(original_file) as input:\n",
        "  with open(tokenized_file, \"w\") as output:\n",
        "    curr_bio = []\n",
        "    for line in input:\n",
        "      line_stripped = line.strip()\n",
        "      if line_stripped != '':\n",
        "        if line_stripped == \"<start_bio>\":\n",
        "          bio_person = [token for token in word_tokenize(next(input).lower()) if token != \"=\"]\n",
        "          \n",
        "          # Adding name of person as stop word\n",
        "          stop.update(bio_person)\n",
        "        elif line_stripped == \"<end_bio>\":\n",
        "          updated = ' '.join([token for token in curr_bio])\n",
        "          output.write(updated)\n",
        "          print(updated)\n",
        "          curr_bio = []\n",
        "\n",
        "          # Removing name of person as stop word\n",
        "          stop -= set(bio_person)\n",
        "          break\n",
        "        else:\n",
        "          curr_bio.extend([token for token in word_tokenize(line_stripped.lower()) if token not in stop])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFyuosfYC6Om",
        "outputId": "4dca8f15-b9fa-4b87-e9d7-301a740f7723"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 october 1781 – 25 september 1860 swiss botanist born strasbourg studied medicine stuttgart bonn dissertation plants collected geofried trier 1787. sent natural history collection london natural history society belgium became plant pathologist started work taxonomist botanical gardens botanical garden strasbourg 1808 gave long lecture lepidoptera switzerland another genus pezizella 1813 latter work contains catalogue approximately 5,000 plants numbers yet calculated also wrote many plant families including olive-flower pisum excelsa russian goldenrod fruticosa pallidum old-growth laurel lagenaria saccharina died strasbourg 25 september 1860 == works == theropoda flower-plant genera swiss canton coimbatore brüssel 1813 swiss flora lombardy containing hundred new species flora caecosphaeria lutrina lymanthella sabionata epipactylia strasbourg 1816 neotropics switzerland three vols trier 1820–1823 london 2nd edition 2 vols 1838–1843 flora schleswig-holstein trier 1835–1838 flora south africa london 1835 iconae plantarum modernarum duo stuttgart 1846 genera swiss flora 2 vols. trier 2nd edition 1857–1859\n"
          ]
        }
      ]
    }
  ]
}