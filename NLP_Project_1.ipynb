{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnshulH/NLP-DL-Group2/blob/fnn/NLP_Project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7U7h5_g6HQj",
        "outputId": "38290311-fa05-4202-c91c-c74cbfc6ee08",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP-DL-Group2'...\n",
            "remote: Enumerating objects: 93, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 93 (delta 18), reused 19 (delta 5), pack-reused 46\u001b[K\n",
            "Unpacking objects: 100% (93/93), 30.78 MiB | 5.43 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!rm -r /content/NLP-DL-Group2/\n",
        "!cd /content && git clone -b fnn https://github.com/AnshulH/NLP-DL-Group2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xOWdVCUmfJZ5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import sys\n",
        "import argparse\n",
        "import os\n",
        "import re\n",
        "from nltk.util import ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0E1tgVr_fJZ6"
      },
      "outputs": [],
      "source": [
        "def decode(vocab,corpus):\n",
        "    \n",
        "    text = ''\n",
        "    for i in range(len(corpus)):\n",
        "        wID = corpus[i]\n",
        "        text = text + vocab[wID] + ' '\n",
        "    return(text)\n",
        "\n",
        "def encode(words,text):\n",
        "    corpus = []\n",
        "    tokens = text.split(' ')\n",
        "    for t in tokens:\n",
        "        try:\n",
        "            wID = words[t][0]\n",
        "        except:\n",
        "            wID = words['<unk>'][0]\n",
        "        corpus.append(wID)\n",
        "    return(corpus)\n",
        "\n",
        "def read_encode(file_name,vocab,words,corpus,threshold):\n",
        "    \n",
        "    wID = len(vocab)\n",
        "    \n",
        "    if threshold > -1:\n",
        "        with open(file_name,'rt', encoding='utf8') as f:\n",
        "            for line in f:\n",
        "                line = line.replace('\\n','')\n",
        "                # Added lower-casing\n",
        "                line = line.lower()\n",
        "                \n",
        "                # Strips out all charcters other than alphanumeric\n",
        "                line = re.sub('[\\W_]+', ' ', line, flags=re.UNICODE)\n",
        "                \n",
        "                # Strips out numbers\n",
        "                line = re.sub('\\d+', '', line)\n",
        "                \n",
        "                tokens = line.split(' ')\n",
        "                for t in tokens:\n",
        "                    try:\n",
        "                        elem = words[t]\n",
        "                    except:\n",
        "                        elem = [wID,0]\n",
        "                        vocab.append(t)\n",
        "                        wID = wID + 1\n",
        "                    elem[1] = elem[1] + 1\n",
        "                    words[t] = elem\n",
        "\n",
        "        temp = words\n",
        "        words = {}\n",
        "        vocab = []\n",
        "        wID = 0\n",
        "        words['<unk>'] = [wID,100]\n",
        "        vocab.append('<unk>')\n",
        "        for t in temp:\n",
        "            if temp[t][1] >= threshold:\n",
        "                vocab.append(t)\n",
        "                wID = wID + 1\n",
        "                words[t] = [wID,temp[t][1]]\n",
        "            \n",
        "                    \n",
        "    with open(file_name,'rt', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            line = line.replace('\\n','')\n",
        "            tokens = line.split(' ')\n",
        "            for t in tokens:\n",
        "                try:\n",
        "                    wID = words[t][0]\n",
        "                except:\n",
        "                    wID = words['<unk>'][0]\n",
        "                corpus.append(wID)\n",
        "                \n",
        "    return [vocab,words,corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4YDBcnYft_l",
        "outputId": "6e4f89eb-7d33-414b-a794-651e3551a349"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab: 29406 train: 3275943\n",
            "vocab: 29406 test: 424931\n"
          ]
        }
      ],
      "source": [
        "params = {\n",
        "        'd_model': 100,\n",
        "        'd_hidden': 100,\n",
        "        'n_layers': 2,\n",
        "        'batch_size': 20,\n",
        "        'seq_len': 30,\n",
        "        'printevery': 5000,\n",
        "        'window': 3,\n",
        "        'epochs': 20,\n",
        "        'lr': 0.0001,\n",
        "        'dropout': 0.35,\n",
        "        'clip': 2.0,\n",
        "        'model': 'FFNN',\n",
        "        'savename': 'lstm',\n",
        "        'loadname': None,\n",
        "        'trainname': '/content/NLP-DL-Group2/hw#1/fake.train.txt',\n",
        "        'validname': '/content/NLP-DL-Group2/hw#1/fake.valid.txt',\n",
        "        'testname': '/content/NLP-DL-Group2/hw#1/fake.test.txt'\n",
        "    }\n",
        "torch.manual_seed(0)\n",
        "\n",
        "[vocab,words,train] = read_encode(params['trainname'],[],{},[],3)\n",
        "print('vocab: %d train: %d' % (len(vocab), len(train)))\n",
        "[vocab,words,test] = read_encode(params['testname'], vocab,words,[],-1)\n",
        "print('vocab: %d test: %d' % (len(vocab),len(test)))\n",
        "params['vocab_size'] = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "QlXXhar6gIZD"
      },
      "outputs": [],
      "source": [
        "# Returns bios in [(bio without puncutation, label), ...]\n",
        "# 0: FAKE\n",
        "# 1: REAL\n",
        "def read_bios(file_name, labels=False):\n",
        "    with open(file_name,'rt', encoding='utf8') as f:\n",
        "        all_bios = f.readlines()\n",
        "        \n",
        "    split_bios = []\n",
        "    curr_bio = \"\"\n",
        "    curr_index = 0\n",
        "    while curr_index < len(all_bios):\n",
        "        curr_line = all_bios[curr_index].lower()\n",
        "        # Strips out all charcters other than alphanumeric\n",
        "        curr_line = re.sub('[\\W_]+', ' ', curr_line, flags=re.UNICODE)\n",
        "        \n",
        "        # Strips out numbers\n",
        "        curr_line = re.sub('\\d+', '', curr_line)\n",
        "        \n",
        "        curr_line = curr_line.strip()\n",
        "        \n",
        "        if curr_line == \"start bio\":\n",
        "            # Skips their name\n",
        "            curr_index += 1\n",
        "        \n",
        "        elif curr_line == \"end bio\":\n",
        "          if labels:\n",
        "            curr_index += 2\n",
        "            if \"FAKE\" in all_bios[curr_index]:\n",
        "                label = 0\n",
        "            else:\n",
        "                label = 1\n",
        "\n",
        "            split_bios.append((curr_bio, label))\n",
        "            curr_bio = \"\"\n",
        "          \n",
        "          else:\n",
        "            split_bios.append((curr_bio, 0))\n",
        "            curr_bio = \"\"\n",
        "        \n",
        "        else:\n",
        "            # Check to ensure not empty space\n",
        "            if curr_line:\n",
        "                if curr_bio == \"\":\n",
        "                    curr_bio = curr_line\n",
        "                else:\n",
        "                    curr_bio += \" \" + curr_line\n",
        "        \n",
        "        curr_index += 1\n",
        "        \n",
        "    \n",
        "    return split_bios\n",
        "    \n",
        "split_bios = read_bios(params['trainname'], False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "cxU-2zc_gJ5A"
      },
      "outputs": [],
      "source": [
        "# Create windows\n",
        "# split_bios: [(bio without puncutation, label), ...]\n",
        "# Returns sliding windows (multiple per biography):\n",
        "# [\n",
        "#  [\n",
        "#   (['Hildebrand', 'Bothe', 'October'], 'September'),\n",
        "#   (['Bothe', 'October', 'September'], 'was'),\n",
        "#  ],\n",
        "#  [\n",
        "#   ([Hermann', 'Robert', 'Kaiser'], 'September'),\n",
        "#   ...\n",
        "#  ]\n",
        "# ]\n",
        "\n",
        "# Reference: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
        "def create_windows(split_bios, window_size):\n",
        "    sliding_windows = []\n",
        "    for bio, label in split_bios:\n",
        "        bio_without_nums = ''.join([i for i in bio if not i.isdigit()])\n",
        "        tokens = [token for token in bio_without_nums.split(\" \") if token != \"\"]\n",
        "        \n",
        "        ngrams = []\n",
        "        for i in range(len(tokens) - window_size):\n",
        "            ngrams.append((\n",
        "                [tokens[i + j] for j in range(window_size)],\n",
        "                tokens[i + window_size]\n",
        "            ))\n",
        "        \n",
        "        sliding_windows.append((ngrams, label))\n",
        "    \n",
        "    return sliding_windows\n",
        "\n",
        "windows = create_windows(split_bios, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eWA2aWKAgLa7"
      },
      "outputs": [],
      "source": [
        "def create_context_and_next_words(windows):\n",
        "  all_context = []\n",
        "  all_next_words = []\n",
        "  skipped_labels = 0\n",
        "\n",
        "  for (each_bio, bio_label) in windows:\n",
        "      for context, label in each_bio:\n",
        "          found_in_words = [word in words for word in context]\n",
        "          found_in_words.extend([label in words])\n",
        "          if all(found_in_words):\n",
        "              all_context.append([words[word][0] for word in context])\n",
        "              all_next_words.append([words[label][0]])\n",
        "          else:\n",
        "              all_context.append([0] * len(context))\n",
        "              all_next_words.append([0])\n",
        "              \n",
        "              skipped_labels += 1\n",
        "\n",
        "  return torch.LongTensor(all_context), torch.LongTensor(all_next_words)\n",
        "\n",
        "# Skipping certain sliding windows because they weren't found in the dictionary\n",
        "# Unk'd?\n",
        "# print(skipped_labels)\n",
        "\n",
        "all_context, all_next_words = create_context_and_next_words(windows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "0ceIec6IgSrj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = TensorDataset(all_context, all_next_words)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_irXR35lui0",
        "outputId": "317020db-f08a-40bd-8802-8f334292b190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GB_oPsO4xrxH"
      },
      "outputs": [],
      "source": [
        "class FFNN(nn.Module):\n",
        "    # d_model = embedding dimensions\n",
        "    def __init__(self, vocab, words,d_model, d_hidden, dropout):\n",
        "        super().__init__() \n",
        "    \n",
        "        self.vocab = vocab\n",
        "        self.words = words\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.d_model = d_model\n",
        "        self.d_hidden = d_hidden\n",
        "        self.dropout = dropout\n",
        "        self.embeds = nn.Embedding(self.vocab_size,d_model)\n",
        "        \n",
        "        # Context size * dimensions for input\n",
        "        # Hidden layer neurons was more difficult to find information\n",
        "        self.linear1 = nn.Linear(3 * d_model, 512)\n",
        "        self.linear2 = nn.Linear(512, self.vocab_size)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embeds = self.embeds(src).view((BATCH_SIZE, -1))\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "    \n",
        "    def lookup(self, src):\n",
        "        embeds = self.embeds(src).flatten(1, 2)\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "                \n",
        "    def init_weights(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MefYYvncgN54",
        "outputId": "ea700982-02a0-46ba-a407-323a7cb9cc2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  batch 200 loss: 6.006665906906128\n",
            "Epoch 0/10: Loss 5.9742\n",
            "  batch 200 loss: 5.835290379524231\n",
            "Epoch 1/10: Loss 6.8649\n",
            "  batch 200 loss: 5.731096076965332\n",
            "Epoch 2/10: Loss 5.7215\n",
            "  batch 200 loss: 5.641232576370239\n",
            "Epoch 3/10: Loss 6.0436\n",
            "  batch 200 loss: 5.634492518901825\n",
            "Epoch 4/10: Loss 5.8620\n",
            "  batch 200 loss: 5.567215499877929\n",
            "Epoch 5/10: Loss 6.4185\n",
            "  batch 200 loss: 5.555097959041595\n",
            "Epoch 6/10: Loss 6.1722\n",
            "  batch 200 loss: 5.546837389469147\n",
            "Epoch 7/10: Loss 6.0870\n",
            "  batch 200 loss: 5.586668076515198\n",
            "Epoch 8/10: Loss 5.8201\n",
            "  batch 200 loss: 5.4718432700634\n",
            "Epoch 9/10: Loss 5.0064\n",
            "Finished!\n"
          ]
        }
      ],
      "source": [
        "# AFTER BATCHING\n",
        "\n",
        "model = FFNN(vocab, words, d_model=100, d_hidden=100, dropout=0.1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "checkpoint = torch.load('/content/gdrive/My Drive/fnn.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "# model = FFNN(vocab, words, d_model=100, d_hidden=100, dropout=0.1)\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    running_loss = 0\n",
        "    for i, (context, label) in enumerate(dataloader):\n",
        "        log_probabilities = model(context)\n",
        "        # Collapsing labels to correct dimensions\n",
        "        label = label.squeeze()\n",
        "        loss = loss_function(log_probabilities, label)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        model.zero_grad()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 200 == 199:\n",
        "            last_loss = running_loss / 200 # loss per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            running_loss = 0.\n",
        "            \n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'batch': i,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                }, \"/content/gdrive/My Drive/fnn.pt\")\n",
        "            \n",
        "            break\n",
        "        \n",
        "    print(\"Epoch {}/{}: Loss {:.4f}\".format(epoch, 10, loss))\n",
        "    \n",
        "print(\"Finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FFNN(vocab, words, d_model=100, d_hidden=100, dropout=0.1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "checkpoint = torch.load('/content/gdrive/My Drive/fnn.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlKVV6p8P47I",
        "outputId": "a377ef99-9ced-4497-825e-27158b5c713e"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FFNN(\n",
              "  (embeds): Embedding(29406, 100)\n",
              "  (linear1): Linear(in_features=300, out_features=512, bias=True)\n",
              "  (linear2): Linear(in_features=512, out_features=29406, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "WdoyNaubyhkm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "split_bios = read_bios('/content/NLP-DL-Group2/hw#1/mix.train.txt', True)\n",
        "windows = create_windows(split_bios, 3)\n",
        "\n",
        "all_sliding_windows = [item[0] for item in windows]\n",
        "all_bio_labels = [item[1] for item in windows]\n",
        "\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Makes context + true words for each given bio\n",
        "# @param: pass it the sliding windows of a given bio\n",
        "# sliding window is a list of pairs\n",
        "# [(['hildebrand', 'bothe', 'october'], 'september'), (['bothe', 'october', 'september'], 'was'), ...]\n",
        "\n",
        "def make_context_and_true_words_per_bio(bio):\n",
        "  all_context = []\n",
        "  all_next_words = []\n",
        "\n",
        "  for context, label in bio:\n",
        "      found_in_words = [word in words for word in context]\n",
        "      found_in_words.extend([label in words])\n",
        "      if all(found_in_words):\n",
        "          all_context.append([words[word][0] for word in context])\n",
        "          all_next_words.append([words[label][0]])\n",
        "      else:\n",
        "          all_context.append([0] * len(context))\n",
        "          all_next_words.append([0])\n",
        "\n",
        "  return torch.LongTensor(all_context), torch.LongTensor(all_next_words)"
      ],
      "metadata": {
        "id": "1299MIfZHfsE"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# probabilites_fake = torch.FloatTensor([])\n",
        "probabilites_fake = []\n",
        "# probabilites_true = torch.FloatTensor([])\n",
        "probabilites_true = []\n",
        "\n",
        "for i in range(len(all_sliding_windows)):\n",
        "  all_sliding_windows_for_bio = all_sliding_windows[i]\n",
        "  context, true_words = make_context_and_true_words_per_bio(all_sliding_windows_for_bio)\n",
        "\n",
        "  bio_label = all_bio_labels[i]\n",
        "\n",
        "  log_probability_table = model.lookup(context)\n",
        "\n",
        "  curr_probabilities = torch.FloatTensor([])\n",
        "\n",
        "  for idx_curr_context, curr_context in enumerate(log_probability_table):\n",
        "    true_word_for_curr_context = true_words[idx_curr_context]\n",
        "    probability_for_true_word = curr_context[true_word_for_curr_context]\n",
        "    curr_probabilities = torch.cat([curr_probabilities, probability_for_true_word])\n",
        "\n",
        "  curr_probabilities = torch.Tensor.mean(curr_probabilities).tolist()\n",
        "\n",
        "  if bio_label == 1:\n",
        "    if len(probabilites_true) < 100:\n",
        "      # probabilites_true = torch.stack([curr_probabilities, probabilites_true])\n",
        "      probabilites_true.append(curr_probabilities)\n",
        "  else:\n",
        "    if len(probabilites_fake) < 100:\n",
        "      # probabilites_fake = torch.stack([curr_probabilities, probabilites_fake])\n",
        "      probabilites_fake.append(curr_probabilities)\n",
        "\n",
        "  # print(len(probabilites_fake), len(probabilites_true))\n",
        "  \n",
        "  if len(probabilites_fake) == 100 and len(probabilites_true) == 100:\n",
        "    break"
      ],
      "metadata": {
        "id": "LM3vj3vayx_9"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.style.use('seaborn-deep')\n",
        "\n",
        "x = probabilites_true\n",
        "y = probabilites_fake\n",
        "bins = np.linspace(-10, 1, 30)\n",
        "\n",
        "plt.hist([x, y], bins, label=['true', 'fake'])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "av2hyyxES1Fu",
        "outputId": "51a67e3e-7fc9-4b05-e0db-bdf951a63775"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}