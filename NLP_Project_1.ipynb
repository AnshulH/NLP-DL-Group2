{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnshulH/NLP-DL-Group2/blob/fnn/NLP_Project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7U7h5_g6HQj",
        "outputId": "4f6d095e-c125-43a0-f935-f756df73d39c",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/NLP-DL-Group2/': No such file or directory\n",
            "Cloning into 'NLP-DL-Group2'...\n",
            "remote: Enumerating objects: 102, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 102 (delta 24), reused 18 (delta 5), pack-reused 46\u001b[K\n",
            "Receiving objects: 100% (102/102), 30.83 MiB | 5.73 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -r /content/NLP-DL-Group2/\n",
        "!cd /content && git clone -b fnn https://github.com/AnshulH/NLP-DL-Group2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xOWdVCUmfJZ5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import sys\n",
        "import argparse\n",
        "import os\n",
        "import re\n",
        "from nltk.util import ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0E1tgVr_fJZ6"
      },
      "outputs": [],
      "source": [
        "def decode(vocab,corpus):\n",
        "    \n",
        "    text = ''\n",
        "    for i in range(len(corpus)):\n",
        "        wID = corpus[i]\n",
        "        text = text + vocab[wID] + ' '\n",
        "    return(text)\n",
        "\n",
        "def encode(words,text):\n",
        "    corpus = []\n",
        "    tokens = text.split(' ')\n",
        "    for t in tokens:\n",
        "        try:\n",
        "            wID = words[t][0]\n",
        "        except:\n",
        "            wID = words['<unk>'][0]\n",
        "        corpus.append(wID)\n",
        "    return(corpus)\n",
        "\n",
        "def read_encode(file_name,vocab,words,corpus,threshold):\n",
        "    \n",
        "    wID = len(vocab)\n",
        "    \n",
        "    if threshold > -1:\n",
        "        with open(file_name,'rt', encoding='utf8') as f:\n",
        "            for line in f:\n",
        "                line = line.replace('\\n','')\n",
        "                # Added lower-casing\n",
        "                line = line.lower()\n",
        "                \n",
        "                # Strips out all charcters other than alphanumeric\n",
        "                line = re.sub('[\\W_]+', ' ', line, flags=re.UNICODE)\n",
        "                \n",
        "                # Strips out numbers\n",
        "                line = re.sub('\\d+', '', line)\n",
        "                \n",
        "                tokens = line.split(' ')\n",
        "                for t in tokens:\n",
        "                    try:\n",
        "                        elem = words[t]\n",
        "                    except:\n",
        "                        elem = [wID,0]\n",
        "                        vocab.append(t)\n",
        "                        wID = wID + 1\n",
        "                    elem[1] = elem[1] + 1\n",
        "                    words[t] = elem\n",
        "\n",
        "        temp = words\n",
        "        words = {}\n",
        "        vocab = []\n",
        "        wID = 0\n",
        "        words['<unk>'] = [wID,100]\n",
        "        vocab.append('<unk>')\n",
        "        for t in temp:\n",
        "            if temp[t][1] >= threshold:\n",
        "                vocab.append(t)\n",
        "                wID = wID + 1\n",
        "                words[t] = [wID,temp[t][1]]\n",
        "            \n",
        "                    \n",
        "    with open(file_name,'rt', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            line = line.replace('\\n','')\n",
        "            tokens = line.split(' ')\n",
        "            for t in tokens:\n",
        "                try:\n",
        "                    wID = words[t][0]\n",
        "                except:\n",
        "                    wID = words['<unk>'][0]\n",
        "                corpus.append(wID)\n",
        "                \n",
        "    return [vocab,words,corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4YDBcnYft_l",
        "outputId": "b5773d5c-140d-4f88-fe78-ed98f8952588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab: 33633 train: 3366260\n",
            "vocab: 33633 test: 441210\n"
          ]
        }
      ],
      "source": [
        "params = {\n",
        "        'd_model': 100,\n",
        "        'd_hidden': 100,\n",
        "        'n_layers': 2,\n",
        "        'batch_size': 20,\n",
        "        'seq_len': 30,\n",
        "        'printevery': 5000,\n",
        "        'window': 3,\n",
        "        'epochs': 20,\n",
        "        'lr': 0.0001,\n",
        "        'dropout': 0.35,\n",
        "        'clip': 2.0,\n",
        "        'model': 'FFNN',\n",
        "        'savename': 'lstm',\n",
        "        'loadname': None,\n",
        "        'trainname': '/content/NLP-DL-Group2/hw#1/mix.train.txt',\n",
        "        'validname': '/content/NLP-DL-Group2/hw#1/mix.valid.txt',\n",
        "        'testname': '/content/NLP-DL-Group2/hw#1/mix.test.txt'\n",
        "    }\n",
        "torch.manual_seed(0)\n",
        "\n",
        "[vocab,words,train] = read_encode(params['trainname'],[],{},[],3)\n",
        "print('vocab: %d train: %d' % (len(vocab), len(train)))\n",
        "[vocab,words,test] = read_encode(params['testname'], vocab,words,[],-1)\n",
        "print('vocab: %d test: %d' % (len(vocab),len(test)))\n",
        "params['vocab_size'] = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QlXXhar6gIZD"
      },
      "outputs": [],
      "source": [
        "# Returns bios in [(bio without puncutation, label), ...]\n",
        "# 0: FAKE\n",
        "# 1: REAL\n",
        "def read_bios(file_name, labels=False):\n",
        "    with open(file_name,'rt', encoding='utf8') as f:\n",
        "        all_bios = f.readlines()\n",
        "        \n",
        "    split_bios = []\n",
        "    curr_bio = \"\"\n",
        "    curr_index = 0\n",
        "    while curr_index < len(all_bios):\n",
        "        curr_line = all_bios[curr_index].lower()\n",
        "        # Strips out all charcters other than alphanumeric\n",
        "        curr_line = re.sub('[\\W_]+', ' ', curr_line, flags=re.UNICODE)\n",
        "        \n",
        "        # Strips out numbers\n",
        "        curr_line = re.sub('\\d+', '', curr_line)\n",
        "        \n",
        "        curr_line = curr_line.strip()\n",
        "        \n",
        "        if curr_line == \"start bio\":\n",
        "            # Skips their name\n",
        "            curr_index += 1\n",
        "        \n",
        "        elif curr_line == \"end bio\":\n",
        "          if labels:\n",
        "            curr_index += 2\n",
        "            if \"FAKE\" in all_bios[curr_index]:\n",
        "                label = 0\n",
        "            else:\n",
        "                label = 1\n",
        "\n",
        "            split_bios.append((curr_bio, label))\n",
        "            curr_bio = \"\"\n",
        "          \n",
        "          else:\n",
        "            split_bios.append((curr_bio, 0))\n",
        "            curr_bio = \"\"\n",
        "        \n",
        "        else:\n",
        "            # Check to ensure not empty space\n",
        "            if curr_line:\n",
        "                if curr_bio == \"\":\n",
        "                    curr_bio = curr_line\n",
        "                else:\n",
        "                    curr_bio += \" \" + curr_line\n",
        "        \n",
        "        curr_index += 1\n",
        "        \n",
        "    \n",
        "    return split_bios\n",
        "    \n",
        "split_bios = read_bios(params['trainname'], True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(split_bios[55])"
      ],
      "metadata": {
        "id": "BUmN1Lw8GzS9",
        "outputId": "82413806-263b-4e06-9451-a06301623691",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('aaron john jack sharp july   november   was an american botanist and bryologist considered an expert on mosses the standard author abbreviation sharp is used to indicate this person as the author when citing a botanical name early life sharp was raised on a dairy farm near east liberty ohio he attended ohio wesleyan university and earned his degree in botany in  after receiving his undergraduate degree sharp was introduced to bryology by george elwood nichols while taking his classes at the university of michigan biological station sharp earned his m s from the university of oklahoma while studying under paul sears in career in  sharp moved to knoxville tennessee and began teaching at the university of tennessee although he was accepted into the ph d program at yale university financial troubles led him to complete his doctorate at ohio state university in  sharp became a full professor at the university of tennessee in  and between  and  he was head of the department of botany sharp served as president of the botanical society of america in  he retired from the university of tennessee in  but remained as an emeritus professor sharp was made fellow of the linnean society in legacy the moss genus unclejackia was named in his honor by daniel h norris two awards bear his name the sharp fund is a monetary award at the university of tennessee for floristic studies in plants and the sharp award of the american bryological and lichenological society is presented to the best student paper at each annual meeting awards honorary doctorate of science ohio wesleyan university merit award of the botanical society of america distinguished professor emeritus university of tennessee elizabeth ann bartholomew award southern appalachian botanical society order of the rising sun rd class japan distinguished service award tennessee environmental education association fellow of the linnean society distinguished achievement citation ohio wesleyan university eloise payne luguer medal garden club of america selected publications sharp aaron john  notas sobre la flora de la regiÃ³n escarpada de la parte noroeste del estado de puebla shanks royal eastman sharp aaron john  summer key to tennessee trees university of tennessee campbell carlos clinton hutson william f sharp aaron john  great smoky mountains wildflowers university of tennessee press isbn fulford margaret sharp aaron john  the leafy hepaticae of mexico one hundred and twenty seven years after c m gottsche new york botanical garden isbn external links ipni list of plant names with authority sharp', 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cxU-2zc_gJ5A"
      },
      "outputs": [],
      "source": [
        "# Create windows\n",
        "# split_bios: [(bio without puncutation, label), ...]\n",
        "# Returns sliding windows (multiple per biography):\n",
        "# [\n",
        "#  [\n",
        "#   (['Hildebrand', 'Bothe', 'October'], 'September'),\n",
        "#   (['Bothe', 'October', 'September'], 'was'),\n",
        "#  ],\n",
        "#  [\n",
        "#   ([Hermann', 'Robert', 'Kaiser'], 'September'),\n",
        "#   ...\n",
        "#  ]\n",
        "# ]\n",
        "\n",
        "# Reference: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
        "def create_windows(split_bios, window_size):\n",
        "    sliding_windows = []\n",
        "    for bio, label in split_bios:\n",
        "        bio_without_nums = ''.join([i for i in bio if not i.isdigit()])\n",
        "        tokens = [token for token in bio_without_nums.split(\" \") if token != \"\"]\n",
        "        \n",
        "        ngrams = []\n",
        "        for i in range(len(tokens) - window_size):\n",
        "            ngrams.append((\n",
        "                [tokens[i + j] for j in range(window_size)],\n",
        "                tokens[i + window_size]\n",
        "            ))\n",
        "        \n",
        "        sliding_windows.append((ngrams, label))\n",
        "    \n",
        "    return sliding_windows\n",
        "\n",
        "windows = create_windows(split_bios, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eWA2aWKAgLa7"
      },
      "outputs": [],
      "source": [
        "def create_context_and_next_words(windows):\n",
        "  all_context = []\n",
        "  all_next_words = []\n",
        "  skipped_labels = 0\n",
        "\n",
        "  for (each_bio, bio_label) in windows:\n",
        "      for context, label in each_bio:\n",
        "          found_in_words = [word in words for word in context]\n",
        "          found_in_words.extend([label in words])\n",
        "          if all(found_in_words):\n",
        "              all_context.append([words[word][0] for word in context])\n",
        "              all_next_words.append([words[label][0]])\n",
        "          else:\n",
        "              all_context.append([0] * len(context))\n",
        "              all_next_words.append([0])\n",
        "              \n",
        "              skipped_labels += 1\n",
        "\n",
        "  return torch.LongTensor(all_context), torch.LongTensor(all_next_words)\n",
        "\n",
        "# Skipping certain sliding windows because they weren't found in the dictionary\n",
        "# Unk'd?\n",
        "# print(skipped_labels)\n",
        "\n",
        "all_context, all_next_words = create_context_and_next_words(windows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0ceIec6IgSrj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = TensorDataset(all_context, all_next_words)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_irXR35lui0",
        "outputId": "d5c699f4-755c-4b46-8797-c48b651aef15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GB_oPsO4xrxH"
      },
      "outputs": [],
      "source": [
        "class FFNN(nn.Module):\n",
        "    # d_model = embedding dimensions\n",
        "    def __init__(self, vocab, words,d_model, d_hidden, dropout):\n",
        "        super().__init__() \n",
        "    \n",
        "        self.vocab = vocab\n",
        "        self.words = words\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.d_model = d_model\n",
        "        self.d_hidden = d_hidden\n",
        "        self.dropout = dropout\n",
        "        self.embeds = nn.Embedding(self.vocab_size,d_model)\n",
        "        \n",
        "        # Context size * dimensions for input\n",
        "        # Hidden layer neurons was more difficult to find information\n",
        "        self.linear1 = nn.Linear(3 * d_model, 512)\n",
        "        self.linear2 = nn.Linear(512, self.vocab_size)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embeds = self.embeds(src).view((BATCH_SIZE, -1))\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "    \n",
        "    def lookup(self, src):\n",
        "        embeds = self.embeds(src).flatten(1, 2)\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "                \n",
        "    def init_weights(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MefYYvncgN54",
        "outputId": "62369689-14b0-4113-8526-333a1d78e236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  batch 200 loss: 6.585859899520874\n",
            "Epoch 0/10: Loss 4.5982\n",
            "  batch 200 loss: 6.00045539855957\n",
            "Epoch 1/10: Loss 6.2290\n",
            "  batch 200 loss: 5.794422476291657\n",
            "Epoch 2/10: Loss 5.6997\n",
            "  batch 200 loss: 5.75495032787323\n",
            "Epoch 3/10: Loss 5.8580\n",
            "  batch 200 loss: 5.663920509815216\n",
            "Epoch 4/10: Loss 4.9840\n",
            "  batch 200 loss: 5.680743398666382\n",
            "Epoch 5/10: Loss 6.1952\n",
            "  batch 200 loss: 5.610845267772675\n",
            "Epoch 6/10: Loss 5.6702\n",
            "  batch 200 loss: 5.638333353996277\n",
            "Epoch 7/10: Loss 5.3960\n",
            "  batch 200 loss: 5.506783947944641\n",
            "Epoch 8/10: Loss 4.9407\n",
            "  batch 200 loss: 5.596408195495606\n",
            "Epoch 9/10: Loss 6.0909\n",
            "Finished!\n"
          ]
        }
      ],
      "source": [
        "# AFTER BATCHING\n",
        "\n",
        "model = FFNN(vocab, words, d_model=100, d_hidden=100, dropout=0.1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# checkpoint = torch.load('/content/gdrive/My Drive/fnn.pt')\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "# model = FFNN(vocab, words, d_model=100, d_hidden=100, dropout=0.1)\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    running_loss = 0\n",
        "    for i, (context, label) in enumerate(dataloader):\n",
        "        log_probabilities = model(context)\n",
        "        # Collapsing labels to correct dimensions\n",
        "        label = label.squeeze()\n",
        "        loss = loss_function(log_probabilities, label)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        model.zero_grad()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % 200 == 199:\n",
        "            last_loss = running_loss / 200 # loss per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            running_loss = 0.\n",
        "            \n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'batch': i,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                }, \"/content/gdrive/My Drive/fnn.pt\")\n",
        "            \n",
        "            break\n",
        "        \n",
        "    print(\"Epoch {}/{}: Loss {:.4f}\".format(epoch, 10, loss))\n",
        "    \n",
        "print(\"Finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FFNN(vocab, words, d_model=100, d_hidden=100, dropout=0.1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "checkpoint = torch.load('/content/gdrive/My Drive/fnn.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlKVV6p8P47I",
        "outputId": "99a90bc7-ff21-4483-9b9e-41f630312944"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FFNN(\n",
              "  (embeds): Embedding(33633, 100)\n",
              "  (linear1): Linear(in_features=300, out_features=512, bias=True)\n",
              "  (linear2): Linear(in_features=512, out_features=33633, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "WdoyNaubyhkm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "split_bios = read_bios('/content/NLP-DL-Group2/hw#1/mix.valid.txt', True)\n",
        "windows = create_windows(split_bios, 3)\n",
        "\n",
        "all_sliding_windows = [item[0] for item in windows]\n",
        "all_bio_labels = [item[1] for item in windows]\n",
        "\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Makes context + true words for each given bio\n",
        "# @param: pass it the sliding windows of a given bio\n",
        "# sliding window is a list of pairs\n",
        "# [(['hildebrand', 'bothe', 'october'], 'september'), (['bothe', 'october', 'september'], 'was'), ...]\n",
        "\n",
        "def make_context_and_true_words_per_bio(bio):\n",
        "  all_context = []\n",
        "  all_next_words = []\n",
        "\n",
        "  for context, label in bio:\n",
        "      found_in_words = [word in words for word in context]\n",
        "      found_in_words.extend([label in words])\n",
        "      if all(found_in_words): \n",
        "          all_context.append([words[word][0] for word in context])\n",
        "          all_next_words.append([words[label][0]])\n",
        "      else:  \n",
        "          all_context.append([0] * len(context))\n",
        "          all_next_words.append([0])\n",
        "\n",
        "  return torch.LongTensor(all_context), torch.LongTensor(all_next_words)"
      ],
      "metadata": {
        "id": "1299MIfZHfsE"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# probabilites_fake = torch.FloatTensor([])\n",
        "probabilites_fake = []\n",
        "# probabilites_true = torch.FloatTensor([])\n",
        "probabilites_true = []\n",
        "\n",
        "for i in range(len(all_sliding_windows)):\n",
        "  all_sliding_windows_for_bio = all_sliding_windows[i]\n",
        "  context, true_words = make_context_and_true_words_per_bio(all_sliding_windows_for_bio)\n",
        "  bio_label = all_bio_labels[i] \n",
        "  log_probability_table = model.lookup(context)\n",
        "\n",
        "  curr_probabilities = torch.FloatTensor([])\n",
        "\n",
        "  for idx_curr_context, curr_context in enumerate(log_probability_table):\n",
        "    true_word_for_curr_context = true_words[idx_curr_context]\n",
        "    probability_for_true_word = curr_context[true_word_for_curr_context]\n",
        "    curr_probabilities = torch.cat([curr_probabilities, probability_for_true_word])\n",
        "\n",
        "  curr_probabilities = torch.Tensor.mean(curr_probabilities).tolist() \n",
        "  if bio_label == 1:\n",
        "    if len(probabilites_true) < 200:\n",
        "      # probabilites_true = torch.stack([curr_probabilities, probabilites_true])\n",
        "      probabilites_true.append(curr_probabilities)\n",
        "  else:\n",
        "    if len(probabilites_fake) < 200:\n",
        "      # probabilites_fake = torch.stack([curr_probabilities, probabilites_fake])\n",
        "      probabilites_fake.append(curr_probabilities)\n",
        "\n",
        "  # print(len(probabilites_fake), len(probabilites_true))\n",
        "  \n",
        "  if len(probabilites_fake) == 200 and len(probabilites_true) == 200:\n",
        "    break"
      ],
      "metadata": {
        "id": "LM3vj3vayx_9"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.style.use('seaborn-deep')\n",
        "\n",
        "x = probabilites_true\n",
        "y = probabilites_fake\n",
        "bins = np.linspace(-10, 1, 30)\n",
        "\n",
        "plt.hist([x, y], bins, label=['true', 'fake'])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "av2hyyxES1Fu",
        "outputId": "16cbef26-e75b-46ed-9480-8830da883ec4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryClassification(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BinaryClassification, self).__init__() \n",
        "        self.layer_1 = nn.Linear(10, 64) \n",
        "        self.layer_2 = nn.Linear(64, 64)\n",
        "        self.layer_out = nn.Linear(64, 10) \n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.1) \n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        x = self.relu(self.layer_1(inputs)) \n",
        "        x = self.relu(self.layer_2(x)) \n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_out(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "XolixInSdG9h"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for training\n",
        "n = len(probabilites_true)\n",
        "probabilites_true.extend(probabilites_fake)\n",
        "y_train = [1]*n\n",
        "n = len(probabilites_fake)\n",
        "y_train.extend([0]*n)"
      ],
      "metadata": {
        "id": "oX-qe3FVeVO1"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelbin = BinaryClassification() \n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "lFg4-aFidjWq"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class TrainData(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data, y_data):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index], self.y_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)"
      ],
      "metadata": {
        "id": "ZzJc4-wEm9ui"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TrainData(torch.FloatTensor(probabilites_true), \n",
        "                       torch.FloatTensor(y_train))"
      ],
      "metadata": {
        "id": "cWgc7NM1ed3b"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset=train_data, batch_size=10, shuffle=True)"
      ],
      "metadata": {
        "id": "frLK37Ileh9q"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "modelbin.train()\n",
        "for e in range(1, EPOCHS+1):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for X_batch, y_batch in train_loader: \n",
        "        optimizer.zero_grad() \n",
        "        y_pred = modelbin(X_batch)  \n",
        "        loss = criterion(y_pred.unsqueeze(1), y_batch.unsqueeze(1)) \n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item() \n",
        "        \n",
        "\n",
        "    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f}')"
      ],
      "metadata": {
        "id": "e6cNfhR4d3nB",
        "outputId": "9899dd03-c258-48cc-bd38-91e95171a591",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001: | Loss: 0.73676\n",
            "Epoch 002: | Loss: 0.75122\n",
            "Epoch 003: | Loss: 0.77162\n",
            "Epoch 004: | Loss: 0.74016\n",
            "Epoch 005: | Loss: 0.76976\n",
            "Epoch 006: | Loss: 0.75717\n",
            "Epoch 007: | Loss: 0.74434\n",
            "Epoch 008: | Loss: 0.75702\n",
            "Epoch 009: | Loss: 0.73884\n",
            "Epoch 010: | Loss: 0.79625\n",
            "Epoch 011: | Loss: 0.76811\n",
            "Epoch 012: | Loss: 0.76860\n",
            "Epoch 013: | Loss: 0.76638\n",
            "Epoch 014: | Loss: 0.75435\n",
            "Epoch 015: | Loss: 0.74507\n",
            "Epoch 016: | Loss: 0.70791\n",
            "Epoch 017: | Loss: 0.74229\n",
            "Epoch 018: | Loss: 0.77835\n",
            "Epoch 019: | Loss: 0.75325\n",
            "Epoch 020: | Loss: 0.76502\n",
            "Epoch 021: | Loss: 0.76900\n",
            "Epoch 022: | Loss: 0.73876\n",
            "Epoch 023: | Loss: 0.72430\n",
            "Epoch 024: | Loss: 0.76905\n",
            "Epoch 025: | Loss: 0.73951\n",
            "Epoch 026: | Loss: 0.76804\n",
            "Epoch 027: | Loss: 0.76479\n",
            "Epoch 028: | Loss: 0.74584\n",
            "Epoch 029: | Loss: 0.77239\n",
            "Epoch 030: | Loss: 0.78826\n",
            "Epoch 031: | Loss: 0.76674\n",
            "Epoch 032: | Loss: 0.74111\n",
            "Epoch 033: | Loss: 0.78468\n",
            "Epoch 034: | Loss: 0.76479\n",
            "Epoch 035: | Loss: 0.73855\n",
            "Epoch 036: | Loss: 0.73234\n",
            "Epoch 037: | Loss: 0.74053\n",
            "Epoch 038: | Loss: 0.76723\n",
            "Epoch 039: | Loss: 0.74682\n",
            "Epoch 040: | Loss: 0.73016\n",
            "Epoch 041: | Loss: 0.76429\n",
            "Epoch 042: | Loss: 0.73232\n",
            "Epoch 043: | Loss: 0.74615\n",
            "Epoch 044: | Loss: 0.78185\n",
            "Epoch 045: | Loss: 0.73307\n",
            "Epoch 046: | Loss: 0.75234\n",
            "Epoch 047: | Loss: 0.74012\n",
            "Epoch 048: | Loss: 0.77430\n",
            "Epoch 049: | Loss: 0.74747\n",
            "Epoch 050: | Loss: 0.75395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for testing\n",
        "n = len(probabilites_true)\n",
        "probabilites_true.extend(probabilites_fake)\n",
        "y_test = [1]*n\n",
        "n = len(probabilites_fake)\n",
        "y_test.extend([0]*n)"
      ],
      "metadata": {
        "id": "VfyUHoHyy38L"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestData():\n",
        "    \n",
        "    def __init__(self, X_data):\n",
        "        self.X_data = X_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)\n",
        "     \n",
        "test_data = TestData(torch.FloatTensor(probabilites_true))"
      ],
      "metadata": {
        "id": "2RsWxznvyTYk"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(dataset=test_data, batch_size=10, shuffle=True)"
      ],
      "metadata": {
        "id": "kaBTEOyqzqyn"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_list = []\n",
        "modelbin.eval()\n",
        "with torch.no_grad():\n",
        "    for X_batch in test_loader: \n",
        "        y_test_pred = modelbin(X_batch)\n",
        "        y_test_pred = torch.sigmoid(y_test_pred)\n",
        "        y_pred_tag = torch.round(y_test_pred)\n",
        "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
        "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
      ],
      "metadata": {
        "id": "yc_bxq2pxA1g"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = []\n",
        "for i in y_pred_list:\n",
        "  for j in i:\n",
        "    y_pred.append(j) "
      ],
      "metadata": {
        "id": "EPsmTwj-3EeM"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "mOIzY2Ikyvnm",
        "outputId": "a25eab28-4c0d-4931-86a1-e73d92ebf3a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[119,  81],\n",
              "       [115,  85]])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}