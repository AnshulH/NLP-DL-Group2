{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnshulH/NLP-DL-Group2/blob/lstm/NLP_Project_1_LSTM_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7U7h5_g6HQj",
        "outputId": "37fba774-5ee5-4608-9be4-1616344ae4b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP-DL-Group2'...\n",
            "remote: Enumerating objects: 63, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 63 (delta 4), reused 6 (delta 1), pack-reused 46\u001b[K\n",
            "Unpacking objects: 100% (63/63), 30.21 MiB | 5.36 MiB/s, done.\n",
            "Updating files: 100% (25/25), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -r /content/NLP-DL-Group2/\n",
        "!cd /content && git clone https://github.com/AnshulH/NLP-DL-Group2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(vocab,corpus):\n",
        "    \n",
        "    text = ''\n",
        "    for i in range(len(corpus)):\n",
        "        wID = corpus[i]\n",
        "        text = text + vocab[wID] + ' '\n",
        "    return(text)\n",
        "\n",
        "def encode(words,text):\n",
        "    corpus = []\n",
        "    tokens = text.split(' ')\n",
        "    for t in tokens:\n",
        "        try:\n",
        "            wID = words[t][0]\n",
        "        except:\n",
        "            wID = words['<unk>'][0]\n",
        "        corpus.append(wID)\n",
        "    return(corpus)\n",
        "\n",
        "def read_encode(file_name,vocab,words,corpus,threshold):\n",
        "    \n",
        "    wID = len(vocab)\n",
        "    \n",
        "    if threshold > -1:\n",
        "        with open(file_name,'rt') as f:\n",
        "            for line in f:\n",
        "                line = line.replace('\\n','')\n",
        "                tokens = line.split(' ')\n",
        "                for t in tokens:\n",
        "                    try:\n",
        "                        elem = words[t]\n",
        "                    except:\n",
        "                        elem = [wID,0]\n",
        "                        vocab.append(t)\n",
        "                        wID = wID + 1\n",
        "                    elem[1] = elem[1] + 1\n",
        "                    words[t] = elem\n",
        "\n",
        "        temp = words\n",
        "        words = {}\n",
        "        vocab = []\n",
        "        wID = 0\n",
        "        words['<unk>'] = [wID,100]\n",
        "        wID = 1\n",
        "        words['<pad>'] = [wID,100]\n",
        "        vocab.append('<unk>')\n",
        "        vocab.append('<pad>')\n",
        "        for t in temp:\n",
        "            if temp[t][1] >= threshold:\n",
        "                vocab.append(t)\n",
        "                wID = wID + 1\n",
        "                words[t] = [wID,temp[t][1]]\n",
        "            \n",
        "                    \n",
        "    with open(file_name,'rt') as f:\n",
        "        for line in f:\n",
        "            line = line.replace('\\n','')\n",
        "            tokens = line.split(' ')\n",
        "            for t in tokens:\n",
        "                try:\n",
        "                    wID = words[t][0]\n",
        "                except:\n",
        "                    wID = words['<unk>'][0]\n",
        "                corpus.append(wID)\n",
        "                \n",
        "    return [vocab,words,corpus]"
      ],
      "metadata": {
        "id": "S92xGxZAejT_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import words # added for detecting English words\n",
        "import string\n",
        "import nltk\n",
        "import calendar\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3BjxK9QyXLA",
        "outputId": "407fdf69-837b-4621-d709-0200ff579fb9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop = set(stopwords.words('english') + list(string.punctuation))"
      ],
      "metadata": {
        "id": "LYnopIlVDnnc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_file = '/content/NLP-DL-Group2/hw#1/mix.train.txt'\n",
        "tokenized_file = '/content/NLP-DL-Group2/hw#1/mix_tokenized.train.txt'\n",
        "\n",
        "with open(original_file) as input:\n",
        "  with open(tokenized_file, \"w\") as output:\n",
        "    curr_bio = []\n",
        "    for line in input:\n",
        "      line_stripped = line.strip()\n",
        "      if line_stripped != '':\n",
        "        if line_stripped == \"\":\n",
        "          bio_person = [token for token in word_tokenize(next(input).lower()) if token != \"=\"]\n",
        "          print(bio_person)\n",
        "          # Adding name of person as stop word\n",
        "          stop.update(bio_person)\n",
        "        elif line_stripped == \"\":\n",
        "          updated = ' '.join([token for token in curr_bio])\n",
        "          output.write(updated)\n",
        "          print(updated)\n",
        "          curr_bio = []\n",
        "\n",
        "          # Removing name of person as stop word\n",
        "          stop -= set(bio_person)\n",
        "          break\n",
        "        else:\n",
        "          curr_bio.extend([token for token in word_tokenize(line_stripped.lower()) if token not in stop])"
      ],
      "metadata": {
        "id": "RClc0OzADqLo"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of tuples containing a list with the tokenized bio and a fake/real label\n",
        "# split_bios = [([bio1, tokens1],[REAL]),([bio2, tokens2],[FAKE])]\n",
        "\n",
        "original_file = '/content/NLP-DL-Group2/hw#1/mix.train.txt'\n",
        "\n",
        "with open(original_file) as input:\n",
        "  all_text = input.readlines()\n",
        "\n",
        "split_bios = []\n",
        "line_text = []\n",
        "\n",
        "# seq_status = 0 marks the start of the bio\n",
        "# seq_status = 1 is the main text of the bio\n",
        "# seq_status = 2 is the end of the bio\n",
        "for line in all_text:\n",
        "  line = line.strip()\n",
        "  if \"<start_bio>\" in line:\n",
        "    seq_status = 0\n",
        "    continue\n",
        "  if \"<end_bio>\" in line:\n",
        "    seq_status = 2\n",
        "    continue\n",
        "  if seq_status == 0:\n",
        "    bio_person = line.strip(\" = \").lower().split()\n",
        "    stop.update(bio_person) # could remove important words if name is meaningful\n",
        "    seq_status = 1\n",
        "    continue\n",
        "  if seq_status == 1:\n",
        "    line = re.sub(\"[\\d-]\", '',line)\n",
        "    line = [token for token in word_tokenize(line.lower()) if token not in stop]\n",
        "    if len(line) > 0:\n",
        "      line_text.extend(line)\n",
        "  if seq_status == 2 and '[' in line:\n",
        "    stop -= set(bio_person)\n",
        "    split_bios.append((line_text,line))\n",
        "    line_text = []\n",
        "\n",
        "print(split_bios[:2])"
      ],
      "metadata": {
        "id": "_oZAoBCxE8Ht",
        "outputId": "82271805-112a-4cd9-8e7a-def0a1baa759",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['october', 'â€“', 'september', 'swiss', 'botanist', 'born', 'strasbourg', 'studied', 'medicine', 'stuttgart', 'bonn', 'dissertation', 'plants', 'collected', 'geofried', 'trier', 'sent', 'natural', 'history', 'collection', 'london', 'natural', 'history', 'society', 'belgium', 'became', 'plant', 'pathologist', 'started', 'work', 'taxonomist', 'botanical', 'gardens', 'botanical', 'garden', 'strasbourg', 'gave', 'long', 'lecture', 'lepidoptera', 'switzerland', 'another', 'genus', 'pezizella', 'latter', 'work', 'contains', 'catalogue', 'approximately', 'plants', 'numbers', 'yet', 'calculated', 'also', 'wrote', 'many', 'plant', 'families', 'including', 'oliveflower', 'pisum', 'excelsa', 'russian', 'goldenrod', 'fruticosa', 'pallidum', 'oldgrowth', 'laurel', 'lagenaria', 'saccharina', 'died', 'strasbourg', 'september', '==', 'works', '==', 'theropoda', 'flowerplant', 'genera', 'swiss', 'canton', 'coimbatore', 'brÃ¼ssel', 'swiss', 'flora', 'lombardy', 'containing', 'hundred', 'new', 'species', 'flora', 'caecosphaeria', 'lutrina', 'lymanthella', 'sabionata', 'epipactylia', 'strasbourg', 'neotropics', 'switzerland', 'three', 'vols', 'trier', 'â€“', 'london', 'nd', 'edition', 'vols', 'â€“', 'flora', 'schleswigholstein', 'trier', 'â€“', 'flora', 'south', 'africa', 'london', 'iconae', 'plantarum', 'modernarum', 'duo', 'stuttgart', 'genera', 'swiss', 'flora', 'vols.', 'trier', 'nd', 'edition', 'â€“'], '[FAKE]'), (['september', 'â€“', 'january', 'german', 'physicist', 'worked', 'kaiserwilhelm', 'institute', 'university', 'jena', 'professor', 'experimental', 'physics', 'worked', 'magnetoacoustic', 'theory', 'ionization', 'theory', 'carried', 'studies', 'magnetic', 'fields', 'principal', 'research', 'highvoltage', 'currents', 'pienza', 'ferromagnetics', 'member', 'german', 'academy', 'sciences.the', 'kaiserwilhelm', 'institute', 'jena', 'high', 'school', 'part', 'university', 'jena', 'named', 'honor', 'many', 'medals', '==', 'publications', '==', 'zwischen', 'magnetism', 'wissenschaftliche', 'rechtsleben', 'band', 'parameter', 'anwendungen', 'Ã¼ber', 'befÃ¶rderung', 'des', 'magneticen', 'vereins', 'wissenschaftliche', 'rechtsleben', 'untersuchungen', 'Ã¼ber', 'die', 'sonstigen', 'absorption', 'und', 'electrodynamik', 'mathematische', 'annalen', 'die', 'brÃ¼ckempferzung', 'eines', 'elektrotechnische', 'buchstimme', 'das', 'infiniertrÃ¼stung', 'des', 'ammerphysikvereins', 'chemie', 'und', 'physik', 'Ã¼ber', 'die', 'magnetiosepenflorzenwahl', 'die', 'Ã¶kologischen', 'krankheiten', 'bei', 'den', 'vogel', 'chemie', 'und', 'physik', 'Ã¼ber', 'die', 'electrochemical', 'kolemnosepenflorzenwahl', 'die', 'Ã¶kologischen', 'krankheiten', 'bei', 'den', 'vogel', 'chemie', 'und', 'physik', 'Ã¼ber', 'die', 'ihre', 'antikelnÃ¼skehrnung', 'einer', 'elektrotechnische', 'phenomen', 'chemie', 'und', 'physik', 'die', 'wandlekontrolle', 'des', 'pienza', 'rods', 'durch', 'einem', 'fachigkeiten', 'chemie', 'und', 'physik', 'Ã¼ber', 'die', 'sein', 'lebennung', 'des', 'infinierts', 'entwurfs', 'grieb', 'mathematische', 'annalen', 'â€“', 'Ã¼ber', 'die', 'zwischen', 'magnetism', 'german', 'anwendungen', 'Ã¼ber', 'befÃ¶rderung', 'des', 'magnetiosepenflorzenwahl', 'die', 'Ã¶kologischen', 'krankheiten', 'bei', 'den', 'vogel', 'chemie', 'und', 'physik', 'Ã¼ber', 'die', 'elektrotechnische', 'zusammenschaft', 'naturforschende', 'zum', 'begrÃ¼ndung', 'von', 'kleine', 'physik', 'alois', 'obermeyer', 'zur', 'elektrodynamik', 'der', 'kÃ¶rper', 'verhandlungen', 'der', 'physik', 'und', 'Ã¤ltere', 'chemie', 'edited', 'karl', 'hermannrafitsch', 'mainz', 'physikalische', 'gesellschaft', 'wien', 'munich', 'mechanik', 'matter', 'eiffel', 'tower'], '[FAKE]')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encodings = read_encode('/content/NLP-DL-Group2/hw#1/mix.train.tok',[],{},[],3)\n",
        "word_dict = encodings[1]\n",
        "vocab_length = max(encodings[2])\n",
        "\n",
        "bio_array = []\n",
        "seq_len = 512\n",
        "y = []\n",
        "for bio in split_bios:\n",
        "  wid = []\n",
        "  for token in bio[0]:\n",
        "    word_info = word_dict.get(token)\n",
        "    if word_info is None:\n",
        "      wid.append(0)\n",
        "    else:\n",
        "      wid.append(word_info[0])\n",
        "  if len(wid) > seq_len:\n",
        "    wid = wid[:seq_len]\n",
        "  elif len(wid) < seq_len:\n",
        "    pad_size = seq_len - len(wid)\n",
        "    wid.extend(np.ones(pad_size,dtype=int))\n",
        "  bio_array.append(np.array(wid))\n",
        "  y.append(word_dict.get(bio[1])[0])\n",
        "\n",
        "list_len = [len(i) for i in bio_array]\n",
        "print(bio_array[:5])\n",
        "print(max(list_len))\n",
        "dummy = torch.tensor(bio_array)\n",
        "y = torch.tensor(y)\n",
        "print(dummy)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3apfnestaJt9",
        "outputId": "ab7f05ff-1c68-4869-cdf2-7f9d7184601b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([ 10,  12,  14,  19,  20,  22,  24,  26,  27,  28,  29,  32,  34,\n",
            "        35,   0,  38,  40,  42,  43,  44,  46,  42,  43,  47,  49,  51,\n",
            "        52,  53,  54,  55,  57,  58,  59,  58,  60,  24,  62,  63,  64,\n",
            "        65,  66,  67,  68,   0,  72,  55,  73,  74,  75,  34,  77,  81,\n",
            "        83,  84,  85,  86,  52,  88,  89,   0,   0,   0,  90,   0,   0,\n",
            "        91,   0,  92,   0,   0,  93,  24,  14,  95,  96,  95,   0,   0,\n",
            "        97,  19,  98,  99,   0,  19, 100, 101, 102, 104, 105, 106, 100,\n",
            "         0,   0,   0,   0,   0,  24,   0,  66, 108, 109,  38,  12,  46,\n",
            "         0, 111, 109,  12, 100,   0,  38,  12, 100, 114, 115,  46,   0,\n",
            "       117,   0, 118,  28,  97,  19, 100, 120,  38,   0, 111,  12,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1]), array([ 14,  12, 129, 131, 132, 134,   0, 136, 137, 138, 139, 140, 141,\n",
            "       134,   0, 142, 143, 142, 144, 146, 147, 148, 150, 151,   0, 153,\n",
            "         0,   0, 154, 131, 155, 156,   0, 136, 138, 157, 158, 160, 137,\n",
            "       138, 161, 162,  86, 164,  95, 165,  95, 166, 167, 168,   0, 169,\n",
            "       170, 171, 172, 173, 174,   0, 175, 168,   0, 176, 172, 177,   0,\n",
            "       178, 179,   0, 180, 181, 177,   0, 184, 185,   0, 186,   0, 174,\n",
            "         0, 187, 179, 188, 172, 177,   0, 177, 192, 193, 194, 195, 196,\n",
            "       187, 179, 188, 172, 177, 197,   0, 177, 192, 193, 194, 195, 196,\n",
            "       187, 179, 188, 172, 177, 198,   0, 199, 185,   0, 187, 179, 188,\n",
            "       177,   0, 174,   0, 200, 201, 202,   0, 187, 179, 188, 172, 177,\n",
            "       204,   0, 174,   0,   0, 206, 180, 181,  12, 172, 177, 166, 167,\n",
            "       131, 171, 172, 173, 174,   0, 177, 192, 193, 194, 195, 196, 187,\n",
            "       179, 188, 172, 177, 185,   0, 209, 211, 212, 213, 214, 188, 216,\n",
            "         0, 217, 218, 219, 220, 221, 219, 188, 179,   0, 187, 222, 223,\n",
            "         0, 224, 225, 226, 227, 228, 229, 230, 231, 232,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1]), array([  236,    12,   238,   240,   241,   242,    55,   244,   245,\n",
            "         246,    55,   247,   248,   249,    96,   250,   247, 19022,\n",
            "         252,    95,   253,    95,    22,     0,   254,   255,     0,\n",
            "       15390,   257,   259,   260,   261,   262,   265,     0,    51,\n",
            "         266,   267,   268,   136,   257,   271,   134,   272,   254,\n",
            "           0,   274,   275,   278,   279,   280,   141,    55,   247,\n",
            "           0,   282,    55,   267,   268,   136,   283,   284,     0,\n",
            "         288,   136,   289,   290,   272,   254,     0,   292,   293,\n",
            "         294,    47,    46,    51,   293,   294,    47,   296,    51,\n",
            "         298,   267,   151,   299,   296,    51,   301,   240,   154,\n",
            "         302,   299,   303,   304,   305,     0,    51,   301,   307,\n",
            "         308,   309,   310,   311,   312,   313,   299,   314,   315,\n",
            "         316,   154,   314,   315,   317,   318,   305,   319,   301,\n",
            "         322,   323,   324,   244,   325,   250,   247,   326,   327,\n",
            "           0,   328,   329,   330,   332,     0,   333,   335,   336,\n",
            "         337,   338,    55,   339,   340,   341,   342,   343,   344,\n",
            "           0,   132,   345,   346,   347,   348,   250,   247,   350,\n",
            "           0,    51,   154,   240,   155,   352,   316,   310,   354,\n",
            "         355,   356,   357,    47,    46,   359,     0,   151,   247,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1]), array([    0,    10,    12,   238,   366,   367,   367,   368,   134,\n",
            "         370,   371, 28579,   373,     0,   373,   374,   375,   376,\n",
            "         377,    96,   378,   379,   380,   381,   382,   383,   385,\n",
            "         386,   134,   370,   371,   387,   388,   389,   390,     0,\n",
            "          22,   391,   379,   393,   394,   395,   396,   371,   397,\n",
            "         137,   379,   398,    51,   137,   379,   396,   401,   402,\n",
            "         137,   403,   158,   370,   404,   406,   408,   409,   370,\n",
            "         410,     0,   278,   411,   370,   412,   402,   137,    95,\n",
            "          96,    95,     0,    85,     0,     0,   415,   416,   417,\n",
            "         418,   386,   419,   420,     0,   375,   421,   401,   422,\n",
            "         423,   424,   421,   428,     0,   430,   432,   373,   433,\n",
            "         435,   436,   424,   437,   393,   438,   439,     0,   440,\n",
            "           0,   441,   442,   444,   373,   445,   446,   447,   447,\n",
            "         242,   373,   449,   451,   452,   453,   424,   454,   455,\n",
            "           0,   456,   457,   373,   248,   446,   459,   461,   462,\n",
            "         464,   373,   465,   466,   467,     0,   466,   471,   473,\n",
            "         474,     0,     0,   476,   386,   477,    55,   478,   479,\n",
            "           0,   449,     0,     0,   354,   481,   482,   370,   412,\n",
            "         137,   483,   484,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "           1,     1,     1,     1,     1,     1,     1,     1]), array([485,   0, 489,   0,  12, 236, 493, 301,   0, 494, 495, 496,   0,\n",
            "       498,   0,   0, 493,   0,   0,   0, 500, 501,   0,  95, 502, 503,\n",
            "        95, 504, 505, 506, 507, 316, 508, 446, 509, 511, 512, 513,   0,\n",
            "       485,  84, 514, 515, 383, 275, 517, 518, 519, 521, 485, 522, 242,\n",
            "       523, 493, 524, 525, 526, 105, 526, 527, 528, 529, 530, 531, 532,\n",
            "       493, 160, 533, 534, 485, 278, 533, 535, 393, 446, 536,  95, 512,\n",
            "       537,  95, 485, 387, 513,   0, 539, 540, 394, 541, 485, 513, 311,\n",
            "       542, 543, 501, 544,  14,  12,  14, 387, 506, 548,   0, 549,   0,\n",
            "         0, 551,   0, 494, 552,   0, 549, 554,   0, 551, 552, 556,  14,\n",
            "        12, 559,   0, 559,  12, 236, 387, 485, 564,   0, 513, 565,  12,\n",
            "       566, 387, 568,   0, 489, 570,  12, 565, 570, 494, 568, 573,   0,\n",
            "       574, 500, 575,   0,   0, 577,  12, 580, 582, 566,  12, 565, 387,\n",
            "       586, 496,   0, 587, 588, 489,  12, 129, 387, 592, 593,   0, 594,\n",
            "         0,  12, 238, 596, 494, 506, 597,   0,   0, 598,   0, 559,   0,\n",
            "        12, 565,   0, 495, 496, 594,  12,  14, 543, 603, 559,  12, 238,\n",
            "       607, 608, 238,  12,  10, 387, 554, 612, 613,   0, 451, 614, 612,\n",
            "         0, 575,   0, 506, 548, 580,  12, 129, 387, 501, 587,   0, 129,\n",
            "       618,  12, 238,   0, 451, 620,   0, 489, 618,  12, 129, 618, 501,\n",
            "       622,   0,  10,   0,  12, 129,   0, 501, 622, 624, 451, 507,   0,\n",
            "       507, 565,  12,  10, 485, 548, 580,  12, 129, 575,   0,  14,  12,\n",
            "        14, 387, 506, 568,   0, 489,   0,  12, 236,   0, 536, 494, 552,\n",
            "       556,   0, 575, 634,   0,  95, 635,  95,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
            "         1,   1,   1,   1,   1])]\n",
            "512\n",
            "tensor([[  10,   12,   14,  ...,    1,    1,    1],\n",
            "        [  14,   12,  129,  ...,    1,    1,    1],\n",
            "        [ 236,   12,  238,  ...,    1,    1,    1],\n",
            "        ...,\n",
            "        [   8,  238,   12,  ...,    1,    1,    1],\n",
            "        [   0, 2372,    8,  ...,    1,    1,    1],\n",
            "        [   8,  884,   12,  ...,    1,    1,    1]])\n",
            "tensor([123, 123, 123,  ..., 123, 636, 636])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.ones(4,dtype=int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiAJRgfUl50Q",
        "outputId": "539e5c0e-5994-4a43-cbdb-2dedec8461c5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self,vocab,words,d_model,d_hidden,n_layers,dropout_rate):\n",
        "        super().__init__()  \n",
        "        self.vocab = vocab\n",
        "        self.words = words\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.n_layers = n_layers\n",
        "        self.d_hidden = d_hidden\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(self.vocab_size,d_model)\n",
        "        #self.lstm = nn.LSTM(input_size=300,hidden_size=d_hidden,num_layers=1,batch_first=True,bidirectional=True)\n",
        "        self.drop = dropout_rate\n",
        "        self.fc = nn.Linear(d_hidden, 1) \n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=d_model,\n",
        "            hidden_size=d_hidden,\n",
        "            num_layers=n_layers,\n",
        "            dropout=dropout_rate\n",
        "        ) \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.lin = nn.Linear(512, 1) \n",
        "        \n",
        "    def forward(self,x,prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        print(embed.size())\n",
        "        print(\"k\")\n",
        "        print(prev_state[0].size())\n",
        "        print(prev_state[1].size())\n",
        "        batch_size = x.size(0)\n",
        "        print(batch_size)\n",
        "        output, state = self.lstm(embed, self.detach_hidden(prev_state))\n",
        "        print(\"dha\")\n",
        "        print(output.size()) \n",
        "        print(state[0].size())\n",
        "        print(state[1].size()) \n",
        "        logits = self.fc(output)\n",
        "        print(logits.size())\n",
        "        out = self.relu(logits)\n",
        "        print(out.size())\n",
        "        out = out.view(batch_size, -1)\n",
        "        print(out.size()) \n",
        "        out = self.lin(out)\n",
        "        out = self.sigmoid(out)\n",
        "        print(out.size())\n",
        "        return out, state \n",
        "        # return [preds,h]\n",
        "    \n",
        "    def init_weights(self,sequence_length): \n",
        "        return (torch.zeros(self.n_layers, 512, self.d_hidden),\n",
        "                torch.zeros(self.n_layers, 512, self.d_hidden))\n",
        "    \n",
        "    def detach_hidden(self, hidden):\n",
        "        h, c = hidden\n",
        "        h = h.detach()\n",
        "        c = c.detach()\n",
        "        return (h,c)\n",
        "       \n",
        "\n",
        "        \n",
        "    "
      ],
      "metadata": {
        "id": "xk0bxuwX0LHW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import sys\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "device = \"cpu\"\n",
        "\n",
        "\n",
        "\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, vocab, words,d_model, d_hidden, dropout):\n",
        "        super().__init__() \n",
        "    \n",
        "        self.vocab = vocab\n",
        "        self.words = words\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.d_model = d_model\n",
        "        self.d_hidden = d_hidden\n",
        "        self.dropout = dropout\n",
        "        self.embeds = nn.Embedding(self.vocab_size,d_model)\n",
        "#          {perform other initializations needed for the FFNN}\n",
        "\n",
        "    def forward(self, src):\n",
        "        embeds = self.dropout(self.embeds(src))\n",
        "#          {add code to implement the FFNN}\n",
        "        pass\n",
        "        # return x\n",
        "                \n",
        "    def init_weights(self):\n",
        "      pass\n",
        "#          {perform initializations}\n",
        "             \n",
        "# class LSTM(nn.Module):\n",
        "#     def __init__(self,vocab,words,d_model,d_hidden,n_layers,dropout_rate):\n",
        "#         super().__init__()\n",
        "        \n",
        "#         self.vocab = vocab\n",
        "#         self.words = words\n",
        "#         self.vocab_size = len(self.vocab)\n",
        "#         self.n_layers = n_layers\n",
        "#         self.d_hidden = d_hidden\n",
        "#         self.d_model = d_model\n",
        "#         self.embeds = nn.Embedding(self.vocab_size,d_model)\n",
        "# #          {perform other initializations needed for the LSTM}\n",
        "#         self.lstm = nn.LSTM(d_model, d_hidden, n_layers, dropout=dropout_rate, batch_first=True)\n",
        "#         self.dropout = nn.Dropout(dropout_rate)\n",
        "#         self.fc = nn.Linear(d_hidden, 1)\n",
        "#         self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "#     def forward(self,src,h):\n",
        "#         embeds = self.dropout(self.embeds(src))\n",
        "#         batch_size = src.size(0)\n",
        "#         src = src.long()\n",
        "#         lstm_out, h = self.lstm(embeds, h)\n",
        "#         lstm_out = lstm_out.contiguous().view(-1, self.d_hidden)\n",
        "        \n",
        "#         out = self.dropout(lstm_out)\n",
        "#         out = self.fc(out)\n",
        "#         out = self.sigmoid(out)\n",
        "        \n",
        "#         out = out.view(batch_size, -1)\n",
        "#         out = out[:,-1]\n",
        "#         return out, h\n",
        "    \n",
        "#     def init_weights(self, batch_size):\n",
        "#         weight = next(self.parameters()).data\n",
        "#         hidden = (weight.new(self.n_layers, batch_size, self.d_hidden).zero_().to(device),\n",
        "#                       weight.new(self.n_layers, batch_size, self.d_hidden).zero_().to(device))\n",
        "#         return hidden\n",
        "        \n",
        "    \n",
        "#     def detach_hidden(self, hidden):\n",
        "#       pass\n",
        "# #          {needed for training...}\n",
        "#         # return [hidden, cell]  \n",
        " \n",
        "def main():\n",
        "    \n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-d_model', type=int, default=100)\n",
        "    parser.add_argument('-d_hidden', type=int, default=100)\n",
        "    parser.add_argument('-n_layers', type=int, default=2)\n",
        "    parser.add_argument('-batch_size', type=int, default=20)\n",
        "    parser.add_argument('-seq_len', type=int, default=30)\n",
        "    parser.add_argument('-printevery', type=int, default=5000)\n",
        "    parser.add_argument('-window', type=int, default=3)\n",
        "    parser.add_argument('-epochs', type=int, default=20)\n",
        "    parser.add_argument('-lr', type=float, default=0.0001)\n",
        "    parser.add_argument('-dropout', type=int, default=0.35)\n",
        "    parser.add_argument('-clip', type=int, default=2.0)\n",
        "    parser.add_argument('-model', type=str,default='LSTM')\n",
        "    parser.add_argument('-savename', type=str,default='lstm')\n",
        "    parser.add_argument('-loadname', type=str)\n",
        "    parser.add_argument('-trainname', type=str,default='wiki.train.txt')\n",
        "    parser.add_argument('-validname', type=str,default='wiki.valid.txt')\n",
        "    parser.add_argument('-testname', type=str,default='wiki.test.txt')\n",
        "\n",
        "    params = {\n",
        "        'd_model': 1,\n",
        "        'd_hidden': 1,\n",
        "        'n_layers': 1,\n",
        "        'batch_size': 100,\n",
        "        'seq_len': 512,\n",
        "        'printevery': 5000,\n",
        "        'window': 3,\n",
        "        'epochs': 20,\n",
        "        'lr': 0.0001,\n",
        "        'dropout': 0.35,\n",
        "        'clip': 2.0,\n",
        "        'model': 'LSTM',\n",
        "        'savename': 'lstm',\n",
        "        'loadname': None,\n",
        "        'trainname': '/content/NLP-DL-Group2/hw#1/mix.train.tok',\n",
        "        'validname': '/content/NLP-DL-Group2/hw#1/mix.train.tok',\n",
        "        'testname': '/content/NLP-DL-Group2/hw#1/mix.train.tok'\n",
        "    }\n",
        "    parser.add_argument(\"-f\", required=False)\n",
        "    \n",
        "    # params = parser.parse_args()    \n",
        "    # torch.manual_seed(0)\n",
        "    \n",
        "    [vocab,words,train] = read_encode(params['trainname'],[],{},[],3)\n",
        "    print('vocab: %d train: %d' % (len(vocab),len(train)))\n",
        "    [vocab,words,test] = read_encode(params['testname'],vocab,words,[],-1)\n",
        "    print('vocab: %d test: %d' % (len(vocab),len(test)))\n",
        "    params['vocab_size'] = len(vocab)\n",
        "\n",
        "    train_loader = read_encode(params['trainname'],[],{},[],3)\n",
        "    \n",
        "    if params['model'] == 'FFNN':\n",
        "      pass\n",
        "#          {add code to instantiate the model, train for K epochs and save model to disk}\n",
        "        \n",
        "    if params['model'] == 'LSTM':\n",
        "      model = LSTM(vocab,words,params['d_model'],params['d_hidden'],params['n_layers'],params['dropout']) \n",
        "      model.to(device)\n",
        "\n",
        "      # lr=0.005\n",
        "      # criterion = nn.BCELoss()\n",
        "      # optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "      # epochs = 2\n",
        "      # counter = 0\n",
        "      # print_every = 1000\n",
        "      # clip = 5\n",
        "      # valid_loss_min = np.Inf\n",
        "\n",
        "      # model.train()\n",
        "      # print(\"dha\")\n",
        "      # print(list(train_loader[1].items())[0:15])\n",
        "\n",
        "      # for i in range(epochs):\n",
        "      #   h = model.init_weights(params['batch_size'])\n",
        "        \n",
        "      #   for vocab, labels in oops:\n",
        "      #       counter += 1\n",
        "      #       h = tuple([e.data for e in h])\n",
        "      #       vocab, labels = vocab.to(device), labels.to(device)\n",
        "      #       model.zero_grad()\n",
        "      #       output, h = model(vocab, h)\n",
        "      #       loss = criterion(output.squeeze(), labels.float())\n",
        "      #       loss.backward()\n",
        "      #       nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "      #       optimizer.step()\n",
        "            \n",
        "      #       if counter%print_every == 0:\n",
        "      #           val_h = model.init_hidden(read_encode(params['validname'],[],{},[],3))\n",
        "      #           val_losses = []\n",
        "      #           model.eval()\n",
        "      #           for inp, lab in params['validname']:\n",
        "      #               val_h = tuple([each.data for each in val_h])\n",
        "      #               inp, lab = inp.to(device), lab.to(device)\n",
        "      #               out, val_h = model(inp, val_h)\n",
        "      #               val_loss = criterion(out.squeeze(), lab.float())\n",
        "      #               val_losses.append(val_loss.item())\n",
        "                    \n",
        "      #           model.train()\n",
        "      #           print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
        "      #                 \"Step: {}...\".format(counter),\n",
        "      #                 \"Loss: {:.6f}...\".format(loss.item()),\n",
        "      #                 \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "      #           if np.mean(val_losses) <= valid_loss_min:\n",
        "      #               torch.save(model.state_dict(), './state_dict.pt')\n",
        "      #               print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
        "      #               valid_loss_min = np.mean(val_losses) \n",
        "      criterion = nn.BCEWithLogitsLoss()\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=0.5) \n",
        "\n",
        "      for epoch in range(20):  \n",
        "              state_h, state_c = model.init_weights(512)\n",
        "              print(state_h.size())\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "              y_pred, (state_h, state_c) = model(dummy[:500], (state_h, state_c))\n",
        "              print(y[:2].size())\n",
        "              print(\"j\")\n",
        "              print(y_pred.view(-1).size())\n",
        "              print(y_pred.type())\n",
        "\n",
        "              w = y.type(torch.FloatTensor)\n",
        "              print(w[:2].type()) \n",
        "              loss = criterion(y_pred.view(-1), w[:500])\n",
        "\n",
        "              state_h = state_h.detach()\n",
        "              state_c = state_c.detach()\n",
        "\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              print({ 'epoch': epoch, 'batch': params['batch_size'], 'loss': loss.item() })\n",
        "\n",
        "\n",
        "    if params['model'] == 'FFNN_CLASSIFY':\n",
        "      pass\n",
        "#          {add code to instantiate the model, recall model parameters and perform/learn classification}\n",
        "\n",
        "    if params['model'] == 'LSTM_CLASSIFY':\n",
        "      pass\n",
        "#          {add code to instantiate the model, recall model parameters and perform/learn classification}\n",
        "        \n",
        "    print(params)\n",
        "    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwOAM-Ht6WMu",
        "outputId": "a284c7a8-ae19-4ca9-9d1b-5dbacd832741"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab: 35151 train: 3012820\n",
            "vocab: 35151 test: 3012820\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "k\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "500\n",
            "dha\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512])\n",
            "torch.Size([500, 1])\n",
            "torch.Size([2])\n",
            "j\n",
            "torch.Size([500])\n",
            "torch.FloatTensor\n",
            "torch.FloatTensor\n",
            "{'epoch': 0, 'batch': 100, 'loss': -204.7021026611328}\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "k\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "500\n",
            "dha\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512])\n",
            "torch.Size([500, 1])\n",
            "torch.Size([2])\n",
            "j\n",
            "torch.Size([500])\n",
            "torch.FloatTensor\n",
            "torch.FloatTensor\n",
            "{'epoch': 1, 'batch': 100, 'loss': -380.23870849609375}\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "k\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "500\n",
            "dha\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512])\n",
            "torch.Size([500, 1])\n",
            "torch.Size([2])\n",
            "j\n",
            "torch.Size([500])\n",
            "torch.FloatTensor\n",
            "torch.FloatTensor\n",
            "{'epoch': 2, 'batch': 100, 'loss': -380.23870849609375}\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "k\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "500\n",
            "dha\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512])\n",
            "torch.Size([500, 1])\n",
            "torch.Size([2])\n",
            "j\n",
            "torch.Size([500])\n",
            "torch.FloatTensor\n",
            "torch.FloatTensor\n",
            "{'epoch': 3, 'batch': 100, 'loss': -380.23870849609375}\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "k\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "500\n",
            "dha\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512])\n",
            "torch.Size([500, 1])\n",
            "torch.Size([2])\n",
            "j\n",
            "torch.Size([500])\n",
            "torch.FloatTensor\n",
            "torch.FloatTensor\n",
            "{'epoch': 4, 'batch': 100, 'loss': -380.23870849609375}\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "k\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "500\n",
            "dha\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512])\n",
            "torch.Size([500, 1])\n",
            "torch.Size([2])\n",
            "j\n",
            "torch.Size([500])\n",
            "torch.FloatTensor\n",
            "torch.FloatTensor\n",
            "{'epoch': 5, 'batch': 100, 'loss': -380.23870849609375}\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "k\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "500\n",
            "dha\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512])\n",
            "torch.Size([500, 1])\n",
            "torch.Size([2])\n",
            "j\n",
            "torch.Size([500])\n",
            "torch.FloatTensor\n",
            "torch.FloatTensor\n",
            "{'epoch': 6, 'batch': 100, 'loss': -380.23870849609375}\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "k\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "500\n",
            "dha\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512])\n",
            "torch.Size([500, 1])\n",
            "torch.Size([2])\n",
            "j\n",
            "torch.Size([500])\n",
            "torch.FloatTensor\n",
            "torch.FloatTensor\n",
            "{'epoch': 7, 'batch': 100, 'loss': -380.23870849609375}\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "k\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "500\n",
            "dha\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512])\n",
            "torch.Size([500, 1])\n",
            "torch.Size([2])\n",
            "j\n",
            "torch.Size([500])\n",
            "torch.FloatTensor\n",
            "torch.FloatTensor\n",
            "{'epoch': 8, 'batch': 100, 'loss': -380.23870849609375}\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "k\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "500\n",
            "dha\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512])\n",
            "torch.Size([500, 1])\n",
            "torch.Size([2])\n",
            "j\n",
            "torch.Size([500])\n",
            "torch.FloatTensor\n",
            "torch.FloatTensor\n",
            "{'epoch': 9, 'batch': 100, 'loss': -380.23870849609375}\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "k\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "500\n",
            "dha\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512])\n",
            "torch.Size([500, 1])\n",
            "torch.Size([2])\n",
            "j\n",
            "torch.Size([500])\n",
            "torch.FloatTensor\n",
            "torch.FloatTensor\n",
            "{'epoch': 10, 'batch': 100, 'loss': -380.23870849609375}\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "k\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "500\n",
            "dha\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512])\n",
            "torch.Size([500, 1])\n",
            "torch.Size([2])\n",
            "j\n",
            "torch.Size([500])\n",
            "torch.FloatTensor\n",
            "torch.FloatTensor\n",
            "{'epoch': 11, 'batch': 100, 'loss': -380.23870849609375}\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "k\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "500\n",
            "dha\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512])\n",
            "torch.Size([500, 1])\n",
            "torch.Size([2])\n",
            "j\n",
            "torch.Size([500])\n",
            "torch.FloatTensor\n",
            "torch.FloatTensor\n",
            "{'epoch': 12, 'batch': 100, 'loss': -380.23870849609375}\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "k\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "500\n",
            "dha\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512])\n",
            "torch.Size([500, 1])\n",
            "torch.Size([2])\n",
            "j\n",
            "torch.Size([500])\n",
            "torch.FloatTensor\n",
            "torch.FloatTensor\n",
            "{'epoch': 13, 'batch': 100, 'loss': -380.23870849609375}\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "k\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "500\n",
            "dha\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512])\n",
            "torch.Size([500, 1])\n",
            "torch.Size([2])\n",
            "j\n",
            "torch.Size([500])\n",
            "torch.FloatTensor\n",
            "torch.FloatTensor\n",
            "{'epoch': 14, 'batch': 100, 'loss': -380.23870849609375}\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "k\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "500\n",
            "dha\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512])\n",
            "torch.Size([500, 1])\n",
            "torch.Size([2])\n",
            "j\n",
            "torch.Size([500])\n",
            "torch.FloatTensor\n",
            "torch.FloatTensor\n",
            "{'epoch': 15, 'batch': 100, 'loss': -380.23870849609375}\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "k\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "500\n",
            "dha\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512])\n",
            "torch.Size([500, 1])\n",
            "torch.Size([2])\n",
            "j\n",
            "torch.Size([500])\n",
            "torch.FloatTensor\n",
            "torch.FloatTensor\n",
            "{'epoch': 16, 'batch': 100, 'loss': -380.23870849609375}\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "k\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "500\n",
            "dha\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512])\n",
            "torch.Size([500, 1])\n",
            "torch.Size([2])\n",
            "j\n",
            "torch.Size([500])\n",
            "torch.FloatTensor\n",
            "torch.FloatTensor\n",
            "{'epoch': 17, 'batch': 100, 'loss': -380.23870849609375}\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "k\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "500\n",
            "dha\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512])\n",
            "torch.Size([500, 1])\n",
            "torch.Size([2])\n",
            "j\n",
            "torch.Size([500])\n",
            "torch.FloatTensor\n",
            "torch.FloatTensor\n",
            "{'epoch': 18, 'batch': 100, 'loss': -380.23870849609375}\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "k\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "500\n",
            "dha\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([1, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512, 1])\n",
            "torch.Size([500, 512])\n",
            "torch.Size([500, 1])\n",
            "torch.Size([2])\n",
            "j\n",
            "torch.Size([500])\n",
            "torch.FloatTensor\n",
            "torch.FloatTensor\n",
            "{'epoch': 19, 'batch': 100, 'loss': -380.23870849609375}\n",
            "{'d_model': 1, 'd_hidden': 1, 'n_layers': 1, 'batch_size': 100, 'seq_len': 512, 'printevery': 5000, 'window': 3, 'epochs': 20, 'lr': 0.0001, 'dropout': 0.35, 'clip': 2.0, 'model': 'LSTM', 'savename': 'lstm', 'loadname': None, 'trainname': '/content/NLP-DL-Group2/hw#1/mix.train.tok', 'validname': '/content/NLP-DL-Group2/hw#1/mix.train.tok', 'testname': '/content/NLP-DL-Group2/hw#1/mix.train.tok', 'vocab_size': 35151}\n"
          ]
        }
      ]
    }
  ]
}